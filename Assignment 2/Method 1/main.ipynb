{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and classes for preprocessing the data\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "  def __init__(self, doc_no, doc_text, tokens):\n",
    "    self.doc_no = doc_no\n",
    "    self.doc_text = doc_text\n",
    "    self.tokens = tokens\n",
    "\n",
    "  def __str__(self):\n",
    "    return 'Document Number: ' + self.doc_no + '\\nDocument Text: ' + self.doc_text + '\\nTokens: ' + str(self.tokens) + '\\n'\n",
    "\n",
    "  def to_dict(self):\n",
    "    return {'docno': self.doc_no, 'doctext': self.doc_text, 'tokens': self.tokens, 'text': ' '.join(self.tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform preprocessing on the text\n",
    "def preprocess(file):\n",
    "  with open(file, \"r\") as f:\n",
    "    content = f.read()\n",
    "  documents = re.findall(r'<DOC>(.*?)</DOC>', content, re.DOTALL)\n",
    "  preprocessed_documents = []\n",
    "  for document in documents:\n",
    "    # Get the document number and text\n",
    "    raw_no = re.search(r'<DOCNO>(.*?)</DOCNO>', document, re.DOTALL)\n",
    "    doc_no = raw_no.group(1) if raw_no else ''\n",
    "    raw_text = re.search(r'<TEXT>(.*?)</TEXT>', document, re.DOTALL)\n",
    "    doc_text = raw_text.string if raw_text else ''\n",
    "\n",
    "    # create a document object\n",
    "    doc = Document(doc_no, doc_text, [])\n",
    "    preprocessed_documents.append(doc)\n",
    "  return preprocessed_documents\n",
    "\n",
    "# main function to preprocess a directory of text files\n",
    "def preprocess_directory(directory, num_files=-1):\n",
    "  preprocessed_documents = []\n",
    "  ctr = 0\n",
    "  for filename in os.listdir(directory):\n",
    "    print('Preprocessing file: ', filename)\n",
    "    file = os.path.join(directory, filename)\n",
    "    preprocessed_documents.extend(preprocess(file))\n",
    "    ctr += 1\n",
    "    if ctr == num_files and num_files != -1:\n",
    "      break\n",
    "    \n",
    "  print('preprocessed ', ctr, ' files')\n",
    "  return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file:  AP880212\n",
      "Preprocessing file:  AP880213\n",
      "Preprocessing file:  AP880214\n",
      "Preprocessing file:  AP880215\n",
      "Preprocessing file:  AP880216\n",
      "Preprocessing file:  AP880217\n",
      "Preprocessing file:  AP880218\n",
      "Preprocessing file:  AP880219\n",
      "Preprocessing file:  AP880220\n",
      "Preprocessing file:  AP880221\n",
      "Preprocessing file:  AP880222\n",
      "Preprocessing file:  AP880223\n",
      "Preprocessing file:  AP880224\n",
      "Preprocessing file:  AP880225\n",
      "Preprocessing file:  AP880226\n",
      "Preprocessing file:  AP880227\n",
      "Preprocessing file:  AP880228\n",
      "Preprocessing file:  AP880229\n",
      "Preprocessing file:  AP880301\n",
      "Preprocessing file:  AP880302\n",
      "Preprocessing file:  AP880303\n",
      "Preprocessing file:  AP880304\n",
      "Preprocessing file:  AP880307\n",
      "Preprocessing file:  AP880308\n",
      "Preprocessing file:  AP880309\n",
      "Preprocessing file:  AP880310\n",
      "Preprocessing file:  AP880311\n",
      "Preprocessing file:  AP880312\n",
      "Preprocessing file:  AP880313\n",
      "Preprocessing file:  AP880314\n",
      "Preprocessing file:  AP880315\n",
      "Preprocessing file:  AP880316\n",
      "Preprocessing file:  AP880317\n",
      "Preprocessing file:  AP880318\n",
      "Preprocessing file:  AP880319\n",
      "Preprocessing file:  AP880320\n",
      "Preprocessing file:  AP880321\n",
      "Preprocessing file:  AP880322\n",
      "Preprocessing file:  AP880323\n",
      "Preprocessing file:  AP880324\n",
      "Preprocessing file:  AP880325\n",
      "Preprocessing file:  AP880326\n",
      "Preprocessing file:  AP880327\n",
      "Preprocessing file:  AP880328\n",
      "Preprocessing file:  AP880329\n",
      "Preprocessing file:  AP880330\n",
      "Preprocessing file:  AP880331\n",
      "Preprocessing file:  AP880401\n",
      "Preprocessing file:  AP880402\n",
      "Preprocessing file:  AP880403\n",
      "Preprocessing file:  AP880404\n",
      "Preprocessing file:  AP880405\n",
      "Preprocessing file:  AP880406\n",
      "Preprocessing file:  AP880407\n",
      "Preprocessing file:  AP880408\n",
      "Preprocessing file:  AP880409\n",
      "Preprocessing file:  AP880410\n",
      "Preprocessing file:  AP880411\n",
      "Preprocessing file:  AP880412\n",
      "Preprocessing file:  AP880413\n",
      "Preprocessing file:  AP880414\n",
      "Preprocessing file:  AP880415\n",
      "Preprocessing file:  AP880416\n",
      "Preprocessing file:  AP880417\n",
      "Preprocessing file:  AP880418\n",
      "Preprocessing file:  AP880419\n",
      "Preprocessing file:  AP880420\n",
      "Preprocessing file:  AP880421\n",
      "Preprocessing file:  AP880422\n",
      "Preprocessing file:  AP880423\n",
      "Preprocessing file:  AP880424\n",
      "Preprocessing file:  AP880425\n",
      "Preprocessing file:  AP880426\n",
      "Preprocessing file:  AP880427\n",
      "Preprocessing file:  AP880428\n",
      "Preprocessing file:  AP880429\n",
      "Preprocessing file:  AP880430\n",
      "Preprocessing file:  AP880501\n",
      "Preprocessing file:  AP880502\n",
      "Preprocessing file:  AP880503\n",
      "Preprocessing file:  AP880504\n",
      "Preprocessing file:  AP880505\n",
      "Preprocessing file:  AP880506\n",
      "Preprocessing file:  AP880507\n",
      "Preprocessing file:  AP880508\n",
      "Preprocessing file:  AP880509\n",
      "Preprocessing file:  AP880510\n",
      "Preprocessing file:  AP880511\n",
      "Preprocessing file:  AP880512\n",
      "Preprocessing file:  AP880513\n",
      "Preprocessing file:  AP880514\n",
      "Preprocessing file:  AP880515\n",
      "Preprocessing file:  AP880516\n",
      "Preprocessing file:  AP880517\n",
      "Preprocessing file:  AP880518\n",
      "Preprocessing file:  AP880519\n",
      "Preprocessing file:  AP880520\n",
      "Preprocessing file:  AP880521\n",
      "Preprocessing file:  AP880522\n",
      "Preprocessing file:  AP880523\n",
      "Preprocessing file:  AP880524\n",
      "Preprocessing file:  AP880525\n",
      "Preprocessing file:  AP880526\n",
      "Preprocessing file:  AP880527\n",
      "Preprocessing file:  AP880528\n",
      "Preprocessing file:  AP880529\n",
      "Preprocessing file:  AP880530\n",
      "Preprocessing file:  AP880531\n",
      "Preprocessing file:  AP880601\n",
      "Preprocessing file:  AP880602\n",
      "Preprocessing file:  AP880603\n",
      "Preprocessing file:  AP880604\n",
      "Preprocessing file:  AP880605\n",
      "Preprocessing file:  AP880606\n",
      "Preprocessing file:  AP880607\n",
      "Preprocessing file:  AP880608\n",
      "Preprocessing file:  AP880609\n",
      "Preprocessing file:  AP880610\n",
      "Preprocessing file:  AP880611\n",
      "Preprocessing file:  AP880612\n",
      "Preprocessing file:  AP880613\n",
      "Preprocessing file:  AP880614\n",
      "Preprocessing file:  AP880615\n",
      "Preprocessing file:  AP880616\n",
      "Preprocessing file:  AP880617\n",
      "Preprocessing file:  AP880618\n",
      "Preprocessing file:  AP880619\n",
      "Preprocessing file:  AP880620\n",
      "Preprocessing file:  AP880621\n",
      "Preprocessing file:  AP880622\n",
      "Preprocessing file:  AP880623\n",
      "Preprocessing file:  AP880624\n",
      "Preprocessing file:  AP880625\n",
      "Preprocessing file:  AP880626\n",
      "Preprocessing file:  AP880627\n",
      "Preprocessing file:  AP880628\n",
      "Preprocessing file:  AP880629\n",
      "Preprocessing file:  AP880630\n",
      "Preprocessing file:  AP880701\n",
      "Preprocessing file:  AP880702\n",
      "Preprocessing file:  AP880703\n",
      "Preprocessing file:  AP880704\n",
      "Preprocessing file:  AP880705\n",
      "Preprocessing file:  AP880706\n",
      "Preprocessing file:  AP880707\n",
      "Preprocessing file:  AP880708\n",
      "Preprocessing file:  AP880709\n",
      "Preprocessing file:  AP880710\n",
      "Preprocessing file:  AP880711\n",
      "Preprocessing file:  AP880712\n",
      "Preprocessing file:  AP880713\n",
      "Preprocessing file:  AP880714\n",
      "Preprocessing file:  AP880715\n",
      "Preprocessing file:  AP880716\n",
      "Preprocessing file:  AP880717\n",
      "Preprocessing file:  AP880718\n",
      "Preprocessing file:  AP880719\n",
      "Preprocessing file:  AP880720\n",
      "Preprocessing file:  AP880721\n",
      "Preprocessing file:  AP880722\n",
      "Preprocessing file:  AP880723\n",
      "Preprocessing file:  AP880724\n",
      "Preprocessing file:  AP880725\n",
      "Preprocessing file:  AP880726\n",
      "Preprocessing file:  AP880727\n",
      "Preprocessing file:  AP880728\n",
      "Preprocessing file:  AP880729\n",
      "Preprocessing file:  AP880730\n",
      "Preprocessing file:  AP880731\n",
      "Preprocessing file:  AP880801\n",
      "Preprocessing file:  AP880802\n",
      "Preprocessing file:  AP880803\n",
      "Preprocessing file:  AP880804\n",
      "Preprocessing file:  AP880805\n",
      "Preprocessing file:  AP880806\n",
      "Preprocessing file:  AP880807\n",
      "Preprocessing file:  AP880808\n",
      "Preprocessing file:  AP880809\n",
      "Preprocessing file:  AP880810\n",
      "Preprocessing file:  AP880811\n",
      "Preprocessing file:  AP880812\n",
      "Preprocessing file:  AP880813\n",
      "Preprocessing file:  AP880814\n",
      "Preprocessing file:  AP880815\n",
      "Preprocessing file:  AP880816\n",
      "Preprocessing file:  AP880817\n",
      "Preprocessing file:  AP880818\n",
      "Preprocessing file:  AP880819\n",
      "Preprocessing file:  AP880820\n",
      "Preprocessing file:  AP880821\n",
      "Preprocessing file:  AP880822\n",
      "Preprocessing file:  AP880823\n",
      "Preprocessing file:  AP880824\n",
      "Preprocessing file:  AP880825\n",
      "Preprocessing file:  AP880826\n",
      "Preprocessing file:  AP880827\n",
      "Preprocessing file:  AP880828\n",
      "Preprocessing file:  AP880829\n",
      "Preprocessing file:  AP880830\n",
      "Preprocessing file:  AP880831\n",
      "Preprocessing file:  AP880901\n",
      "Preprocessing file:  AP880902\n",
      "Preprocessing file:  AP880903\n",
      "Preprocessing file:  AP880904\n",
      "Preprocessing file:  AP880905\n",
      "Preprocessing file:  AP880906\n",
      "Preprocessing file:  AP880907\n",
      "Preprocessing file:  AP880908\n",
      "Preprocessing file:  AP880909\n",
      "Preprocessing file:  AP880910\n",
      "Preprocessing file:  AP880911\n",
      "Preprocessing file:  AP880912\n",
      "Preprocessing file:  AP880913\n",
      "Preprocessing file:  AP880914\n",
      "Preprocessing file:  AP880915\n",
      "Preprocessing file:  AP880916\n",
      "Preprocessing file:  AP880917\n",
      "Preprocessing file:  AP880918\n",
      "Preprocessing file:  AP880919\n",
      "Preprocessing file:  AP880920\n",
      "Preprocessing file:  AP880921\n",
      "Preprocessing file:  AP880922\n",
      "Preprocessing file:  AP880923\n",
      "Preprocessing file:  AP880924\n",
      "Preprocessing file:  AP880925\n",
      "Preprocessing file:  AP880926\n",
      "Preprocessing file:  AP880927\n",
      "Preprocessing file:  AP880928\n",
      "Preprocessing file:  AP880929\n",
      "Preprocessing file:  AP880930\n",
      "Preprocessing file:  AP881001\n",
      "Preprocessing file:  AP881002\n",
      "Preprocessing file:  AP881003\n",
      "Preprocessing file:  AP881004\n",
      "Preprocessing file:  AP881005\n",
      "Preprocessing file:  AP881006\n",
      "Preprocessing file:  AP881007\n",
      "Preprocessing file:  AP881008\n",
      "Preprocessing file:  AP881009\n",
      "Preprocessing file:  AP881010\n",
      "Preprocessing file:  AP881011\n",
      "Preprocessing file:  AP881012\n",
      "Preprocessing file:  AP881013\n",
      "Preprocessing file:  AP881014\n",
      "Preprocessing file:  AP881015\n",
      "Preprocessing file:  AP881016\n",
      "Preprocessing file:  AP881017\n",
      "Preprocessing file:  AP881018\n",
      "Preprocessing file:  AP881019\n",
      "Preprocessing file:  AP881020\n",
      "Preprocessing file:  AP881021\n",
      "Preprocessing file:  AP881022\n",
      "Preprocessing file:  AP881023\n",
      "Preprocessing file:  AP881024\n",
      "Preprocessing file:  AP881025\n",
      "Preprocessing file:  AP881026\n",
      "Preprocessing file:  AP881027\n",
      "Preprocessing file:  AP881028\n",
      "Preprocessing file:  AP881029\n",
      "Preprocessing file:  AP881030\n",
      "Preprocessing file:  AP881031\n",
      "Preprocessing file:  AP881101\n",
      "Preprocessing file:  AP881102\n",
      "Preprocessing file:  AP881103\n",
      "Preprocessing file:  AP881104\n",
      "Preprocessing file:  AP881105\n",
      "Preprocessing file:  AP881106\n",
      "Preprocessing file:  AP881107\n",
      "Preprocessing file:  AP881108\n",
      "Preprocessing file:  AP881109\n",
      "Preprocessing file:  AP881110\n",
      "Preprocessing file:  AP881111\n",
      "Preprocessing file:  AP881112\n",
      "Preprocessing file:  AP881113\n",
      "Preprocessing file:  AP881114\n",
      "Preprocessing file:  AP881115\n",
      "Preprocessing file:  AP881116\n",
      "Preprocessing file:  AP881117\n",
      "Preprocessing file:  AP881118\n",
      "Preprocessing file:  AP881119\n",
      "Preprocessing file:  AP881120\n",
      "Preprocessing file:  AP881121\n",
      "Preprocessing file:  AP881122\n",
      "Preprocessing file:  AP881123\n",
      "Preprocessing file:  AP881124\n",
      "Preprocessing file:  AP881125\n",
      "Preprocessing file:  AP881126\n",
      "Preprocessing file:  AP881127\n",
      "Preprocessing file:  AP881128\n",
      "Preprocessing file:  AP881129\n",
      "Preprocessing file:  AP881130\n",
      "Preprocessing file:  AP881201\n",
      "Preprocessing file:  AP881202\n",
      "Preprocessing file:  AP881203\n",
      "Preprocessing file:  AP881204\n",
      "Preprocessing file:  AP881205\n",
      "Preprocessing file:  AP881206\n",
      "Preprocessing file:  AP881207\n",
      "Preprocessing file:  AP881208\n",
      "Preprocessing file:  AP881209\n",
      "Preprocessing file:  AP881210\n",
      "Preprocessing file:  AP881211\n",
      "Preprocessing file:  AP881212\n",
      "Preprocessing file:  AP881213\n",
      "Preprocessing file:  AP881214\n",
      "Preprocessing file:  AP881215\n",
      "Preprocessing file:  AP881216\n",
      "Preprocessing file:  AP881217\n",
      "Preprocessing file:  AP881218\n",
      "Preprocessing file:  AP881219\n",
      "Preprocessing file:  AP881220\n",
      "Preprocessing file:  AP881221\n",
      "Preprocessing file:  AP881222\n",
      "Preprocessing file:  AP881223\n",
      "Preprocessing file:  AP881224\n",
      "Preprocessing file:  AP881225\n",
      "Preprocessing file:  AP881226\n",
      "Preprocessing file:  AP881227\n",
      "Preprocessing file:  AP881228\n",
      "Preprocessing file:  AP881229\n",
      "Preprocessing file:  AP881230\n",
      "Preprocessing file:  AP881231\n",
      "preprocessed  322  files\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the collection\n",
    "preprocessed_documents = preprocess_directory('AP_collection/coll')\n",
    "preprocessed_documents.sort(key=lambda x: x.doc_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the topics from the topics file\n",
    "def extract_topics(file, descriptions=False):\n",
    "  with open(file, \"r\") as f:\n",
    "    topic_content = f.read()\n",
    "  all_topics = []\n",
    "  topics = re.findall(r'<top>(.*?)</top>', topic_content, re.DOTALL)\n",
    "  for topic in topics:\n",
    "    raw_title = re.search(r'<title>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "    title = raw_title.group(1) if raw_title else ''\n",
    "    if descriptions:\n",
    "      raw_desc = re.search(r'<desc>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "      desc = raw_desc.group(1) if raw_desc else ''\n",
    "      all_topics.append({'title': title, 'description': desc})\n",
    "    else:\n",
    "      all_topics.append({'title': title})\n",
    "  return all_topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerSpecb(SentenceTransformer):\n",
    "  # Requires:\n",
    "  # pip install git+https://github.com/Muennighoff/sentence-transformers.git@sgpt_poolings_specb\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    tokens = [\"[SOS]\", \"{SOS}\"]\n",
    "    self._first_module().tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "    self._first_module().auto_model.resize_token_embeddings(len(self._first_module().tokenizer))\n",
    "    # Will be replaced with the rep tokens in the model ones\n",
    "    # The problem is we don't know if a text is query or document when tokenizing in the Transformer.py module, \n",
    "    # so we use the SOS tokens as an identifier if we have a query or document at hand & then replace them\n",
    "    # If we would directly use the brackets here, they may become part of another token\n",
    "    self._first_module().bos_spec_token_q = self._first_module().tokenizer.encode(\"[SOS]\", add_special_tokens=False)[0]\n",
    "    self._first_module().bos_spec_token_d = self._first_module().tokenizer.encode(\"{SOS}\", add_special_tokens=False)[0]\n",
    "    self._first_module().bos_spec_token_q_rep = self._first_module().tokenizer.encode(\"[\", add_special_tokens=False)[0]\n",
    "    self._first_module().eos_spec_token_q = self._first_module().tokenizer.encode(\"]\", add_special_tokens=False)[0]\n",
    "    self._first_module().bos_spec_token_d_rep = self._first_module().tokenizer.encode(\"{\", add_special_tokens=False)[0]\n",
    "    self._first_module().eos_spec_token_d = self._first_module().tokenizer.encode(\"}\", add_special_tokens=False)[0]\n",
    "    self._first_module().replace_bos = True\n",
    "\n",
    "def encode(self, sentences, **kwargs):\n",
    "    is_query = kwargs.pop(\"is_query\", True)\n",
    "    if is_query:\n",
    "        sentences = \"[SOS]\" + sentences if isinstance(sentences, str) else [\"[SOS]\" + sent for sent in sentences]\n",
    "    else:\n",
    "        sentences = \"{SOS}\" + sentences if isinstance(sentences, str) else [\"{SOS}\" + sent for sent in sentences]    \n",
    "    return super().encode(sentences, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "model_name='SGPT-125M-weightedmean-nli-bitfit'\n",
    "model = SentenceTransformerSpecb(f'Muennighoff/{model_name}', device='cuda:0', cache_folder='./.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def search(n, query, model, preprocessed_documents, doc_embeddings, top_k=20):\n",
    "  # Fetch the embeddings for the query if it exists, otherwise compute it\n",
    "  if os.path.exists(f'embedding_saves/{model_name}/query-{n}.pickle'):\n",
    "    query_embeddings = torch.load(f'embedding_saves/{model_name}/query-{n}.pickle')\n",
    "  else:\n",
    "    query_embeddings = model.encode([query])\n",
    "    os.makedirs(f'embedding_saves/{model_name}', exist_ok=True)\n",
    "    torch.save(query_embeddings, f'embedding_saves/{model_name}/query-{n}.pickle')\n",
    "  # compute distances\n",
    "  distances = scipy.spatial.distance.cdist(query_embeddings, doc_embeddings, \"cosine\")[0]\n",
    "  # get the top k results\n",
    "  results = zip(range(len(distances)), distances)\n",
    "  results = sorted(results, key=lambda x: x[1])\n",
    "  # Create a list of tuples with the document number and the distance\n",
    "  results = [(preprocessed_documents[idx].doc_no, distance) for idx, distance in results[0:top_k]]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding AP881231-0060 86/79923...\n",
      "Embedding AP881231-0059 87/79923...\n",
      "Embedding AP881231-0058 88/79923...\n",
      "Embedding AP881231-0057 89/79923...\n",
      "Embedding AP881231-0056 90/79923...\n",
      "Embedding AP881231-0055 91/79923...\n",
      "Embedding AP881231-0054 92/79923...\n",
      "Embedding AP881231-0053 93/79923...\n",
      "Embedding AP881231-0052 94/79923...\n",
      "Embedding AP881231-0051 95/79923...\n",
      "Embedding AP881231-0050 96/79923...\n",
      "Embedding AP881231-0049 97/79923...\n",
      "Embedding AP881231-0048 98/79923...\n",
      "Embedding AP881231-0047 99/79923...\n",
      "Embedding AP881231-0046 100/79923...\n",
      "Embedding AP881231-0045 101/79923...\n",
      "Embedding AP881231-0044 102/79923...\n",
      "Embedding AP881231-0043 103/79923...\n",
      "Embedding AP881231-0042 104/79923...\n",
      "Embedding AP881231-0041 105/79923...\n",
      "Embedding AP881231-0040 106/79923...\n",
      "Embedding AP881231-0039 107/79923...\n",
      "Embedding AP881231-0038 108/79923...\n",
      "Embedding AP881231-0037 109/79923...\n",
      "Embedding AP881231-0036 110/79923...\n",
      "Embedding AP881231-0035 111/79923...\n",
      "Embedding AP881231-0034 112/79923...\n",
      "Embedding AP881231-0033 113/79923...\n",
      "Embedding AP881231-0032 114/79923...\n",
      "Embedding AP881231-0031 115/79923...\n",
      "Embedding AP881231-0030 116/79923...\n",
      "Embedding AP881231-0029 117/79923...\n",
      "Embedding AP881231-0028 118/79923...\n",
      "Embedding AP881231-0027 119/79923...\n",
      "Embedding AP881231-0026 120/79923...\n",
      "Embedding AP881231-0025 121/79923...\n",
      "Embedding AP881231-0024 122/79923...\n",
      "Embedding AP881231-0023 123/79923...\n",
      "Embedding AP881231-0022 124/79923...\n",
      "Embedding AP881231-0021 125/79923...\n",
      "Embedding AP881231-0020 126/79923...\n",
      "Embedding AP881231-0019 127/79923...\n",
      "Embedding AP881231-0018 128/79923...\n",
      "Embedding AP881231-0017 129/79923...\n",
      "Embedding AP881231-0016 130/79923...\n",
      "Embedding AP881231-0015 131/79923...\n",
      "Embedding AP881231-0014 132/79923...\n",
      "Embedding AP881231-0013 133/79923...\n",
      "Embedding AP881231-0012 134/79923...\n",
      "Embedding AP881231-0011 135/79923...\n",
      "Embedding AP881231-0010 136/79923...\n",
      "Embedding AP881231-0009 137/79923...\n",
      "Embedding AP881231-0008 138/79923...\n",
      "Embedding AP881231-0007 139/79923...\n",
      "Embedding AP881231-0006 140/79923...\n",
      "Embedding AP881231-0005 141/79923...\n",
      "Embedding AP881231-0004 142/79923...\n",
      "Embedding AP881231-0003 143/79923...\n",
      "Embedding AP881231-0002 144/79923...\n",
      "Embedding AP881231-0001 145/79923...\n",
      "Embedding AP881230-0236 146/79923...\n",
      "Embedding AP881230-0235 147/79923...\n",
      "Embedding AP881230-0234 148/79923...\n",
      "Embedding AP881230-0233 149/79923...\n",
      "Embedding AP881230-0232 150/79923...\n",
      "Embedding AP881230-0231 151/79923...\n",
      "Embedding AP881230-0230 152/79923...\n",
      "Embedding AP881230-0229 153/79923...\n",
      "Embedding AP881230-0228 154/79923...\n",
      "Embedding AP881230-0227 155/79923...\n",
      "Embedding AP881230-0226 156/79923...\n",
      "Embedding AP881230-0225 157/79923...\n",
      "Embedding AP881230-0224 158/79923...\n",
      "Embedding AP881230-0223 159/79923...\n",
      "Embedding AP881230-0222 160/79923...\n",
      "Embedding AP881230-0221 161/79923...\n",
      "Embedding AP881230-0220 162/79923...\n",
      "Embedding AP881230-0219 163/79923...\n",
      "Embedding AP881230-0218 164/79923...\n",
      "Embedding AP881230-0217 165/79923...\n",
      "Embedding AP881230-0216 166/79923...\n",
      "Embedding AP881230-0215 167/79923...\n",
      "Embedding AP881230-0214 168/79923...\n",
      "Embedding AP881230-0213 169/79923...\n",
      "Embedding AP881230-0212 170/79923...\n",
      "Embedding AP881230-0211 171/79923...\n",
      "Embedding AP881230-0210 172/79923...\n",
      "Embedding AP881230-0209 173/79923...\n",
      "Embedding AP881230-0208 174/79923...\n",
      "Embedding AP881230-0207 175/79923...\n",
      "Embedding AP881230-0206 176/79923...\n",
      "Embedding AP881230-0205 177/79923...\n",
      "Embedding AP881230-0204 178/79923...\n",
      "Embedding AP881230-0203 179/79923...\n",
      "Embedding AP881230-0202 180/79923...\n",
      "Embedding AP881230-0201 181/79923...\n",
      "Embedding AP881230-0200 182/79923...\n",
      "Embedding AP881230-0199 183/79923...\n",
      "Embedding AP881230-0198 184/79923...\n",
      "Embedding AP881230-0197 185/79923...\n",
      "Embedding AP881230-0196 186/79923...\n",
      "Embedding AP881230-0195 187/79923...\n",
      "Embedding AP881230-0194 188/79923...\n",
      "Embedding AP881230-0193 189/79923...\n",
      "Embedding AP881230-0192 190/79923...\n",
      "Embedding AP881230-0191 191/79923...\n",
      "Embedding AP881230-0190 192/79923...\n",
      "Embedding AP881230-0189 193/79923...\n",
      "Embedding AP881230-0188 194/79923...\n",
      "Embedding AP881230-0187 195/79923...\n",
      "Embedding AP881230-0186 196/79923...\n",
      "Embedding AP881230-0185 197/79923...\n",
      "Embedding AP881230-0184 198/79923...\n",
      "Embedding AP881230-0183 199/79923...\n",
      "Embedding AP881230-0182 200/79923...\n",
      "Embedding AP881230-0181 201/79923...\n",
      "Embedding AP881230-0180 202/79923...\n",
      "Embedding AP881230-0179 203/79923...\n",
      "Embedding AP881230-0178 204/79923...\n",
      "Embedding AP881230-0177 205/79923...\n",
      "Embedding AP881230-0176 206/79923...\n",
      "Embedding AP881230-0175 207/79923...\n",
      "Embedding AP881230-0174 208/79923...\n",
      "Embedding AP881230-0173 209/79923...\n",
      "Embedding AP881230-0172 210/79923...\n",
      "Embedding AP881230-0171 211/79923...\n",
      "Embedding AP881230-0170 212/79923...\n",
      "Embedding AP881230-0169 213/79923...\n",
      "Embedding AP881230-0168 214/79923...\n",
      "Embedding AP881230-0167 215/79923...\n",
      "Embedding AP881230-0166 216/79923...\n",
      "Embedding AP881230-0165 217/79923...\n",
      "Embedding AP881230-0164 218/79923...\n",
      "Embedding AP881230-0163 219/79923...\n",
      "Embedding AP881230-0162 220/79923...\n",
      "Embedding AP881230-0161 221/79923...\n",
      "Embedding AP881230-0160 222/79923...\n",
      "Embedding AP881230-0159 223/79923...\n",
      "Embedding AP881230-0158 224/79923...\n",
      "Embedding AP881230-0157 225/79923...\n",
      "Embedding AP881230-0156 226/79923...\n",
      "Embedding AP881230-0155 227/79923...\n",
      "Embedding AP881230-0154 228/79923...\n",
      "Embedding AP881230-0153 229/79923...\n",
      "Embedding AP881230-0152 230/79923...\n",
      "Embedding AP881230-0151 231/79923...\n",
      "Embedding AP881230-0150 232/79923...\n",
      "Embedding AP881230-0149 233/79923...\n",
      "Embedding AP881230-0148 234/79923...\n",
      "Embedding AP881230-0147 235/79923...\n",
      "Embedding AP881230-0146 236/79923...\n",
      "Embedding AP881230-0145 237/79923...\n",
      "Embedding AP881230-0144 238/79923...\n",
      "Embedding AP881230-0143 239/79923...\n",
      "Embedding AP881230-0142 240/79923...\n",
      "Embedding AP881230-0141 241/79923...\n",
      "Embedding AP881230-0140 242/79923...\n",
      "Embedding AP881230-0139 243/79923...\n",
      "Embedding AP881230-0138 244/79923...\n",
      "Embedding AP881230-0137 245/79923...\n",
      "Embedding AP881230-0136 246/79923...\n",
      "Embedding AP881230-0135 247/79923...\n",
      "Embedding AP881230-0134 248/79923...\n",
      "Embedding AP881230-0133 249/79923...\n",
      "Embedding AP881230-0132 250/79923...\n",
      "Embedding AP881230-0131 251/79923...\n",
      "Embedding AP881230-0130 252/79923...\n",
      "Embedding AP881230-0129 253/79923...\n",
      "Embedding AP881230-0128 254/79923...\n",
      "Embedding AP881230-0127 255/79923...\n",
      "Embedding AP881230-0126 256/79923...\n",
      "Embedding AP881230-0125 257/79923...\n",
      "Embedding AP881230-0124 258/79923...\n",
      "Embedding AP881230-0123 259/79923...\n",
      "Embedding AP881230-0122 260/79923...\n",
      "Embedding AP881230-0121 261/79923...\n",
      "Embedding AP881230-0120 262/79923...\n",
      "Embedding AP881230-0119 263/79923...\n",
      "Embedding AP881230-0118 264/79923...\n",
      "Embedding AP881230-0117 265/79923...\n",
      "Embedding AP881230-0116 266/79923...\n",
      "Embedding AP881230-0115 267/79923...\n",
      "Embedding AP881230-0114 268/79923...\n",
      "Embedding AP881230-0113 269/79923...\n",
      "Embedding AP881230-0112 270/79923...\n",
      "Embedding AP881230-0111 271/79923...\n",
      "Embedding AP881230-0110 272/79923...\n",
      "Embedding AP881230-0109 273/79923...\n",
      "Embedding AP881230-0108 274/79923...\n",
      "Embedding AP881230-0107 275/79923...\n",
      "Embedding AP881230-0106 276/79923...\n",
      "Embedding AP881230-0105 277/79923...\n",
      "Embedding AP881230-0104 278/79923...\n",
      "Embedding AP881230-0103 279/79923...\n",
      "Embedding AP881230-0102 280/79923...\n",
      "Embedding AP881230-0101 281/79923...\n",
      "Embedding AP881230-0100 282/79923...\n",
      "Embedding AP881230-0099 283/79923...\n",
      "Embedding AP881230-0098 284/79923...\n",
      "Embedding AP881230-0097 285/79923...\n",
      "Embedding AP881230-0096 286/79923...\n",
      "Embedding AP881230-0095 287/79923...\n",
      "Embedding AP881230-0094 288/79923...\n",
      "Embedding AP881230-0093 289/79923...\n",
      "Embedding AP881230-0092 290/79923...\n",
      "Embedding AP881230-0091 291/79923...\n",
      "Embedding AP881230-0090 292/79923...\n",
      "Embedding AP881230-0089 293/79923...\n",
      "Embedding AP881230-0088 294/79923...\n",
      "Embedding AP881230-0087 295/79923...\n",
      "Embedding AP881230-0086 296/79923...\n",
      "Embedding AP881230-0085 297/79923...\n",
      "Embedding AP881230-0084 298/79923...\n",
      "Embedding AP881230-0083 299/79923...\n",
      "Embedding AP881230-0082 300/79923...\n",
      "Embedding AP881230-0081 301/79923...\n",
      "Embedding AP881230-0080 302/79923...\n",
      "Embedding AP881230-0079 303/79923...\n",
      "Embedding AP881230-0078 304/79923...\n",
      "Embedding AP881230-0077 305/79923...\n",
      "Embedding AP881230-0076 306/79923...\n",
      "Embedding AP881230-0075 307/79923...\n",
      "Embedding AP881230-0074 308/79923...\n",
      "Embedding AP881230-0073 309/79923...\n",
      "Embedding AP881230-0072 310/79923...\n",
      "Embedding AP881230-0071 311/79923...\n",
      "Embedding AP881230-0070 312/79923...\n",
      "Embedding AP881230-0069 313/79923...\n",
      "Embedding AP881230-0068 314/79923...\n",
      "Embedding AP881230-0067 315/79923...\n",
      "Embedding AP881230-0066 316/79923...\n",
      "Embedding AP881230-0065 317/79923...\n",
      "Embedding AP881230-0064 318/79923...\n",
      "Embedding AP881230-0063 319/79923...\n",
      "Embedding AP881230-0062 320/79923...\n",
      "Embedding AP881230-0061 321/79923...\n",
      "Embedding AP881230-0060 322/79923...\n",
      "Embedding AP881230-0059 323/79923...\n",
      "Embedding AP881230-0058 324/79923...\n",
      "Embedding AP881230-0057 325/79923...\n",
      "Embedding AP881230-0056 326/79923...\n",
      "Embedding AP881230-0055 327/79923...\n",
      "Embedding AP881230-0054 328/79923...\n",
      "Embedding AP881230-0053 329/79923...\n",
      "Embedding AP881230-0052 330/79923...\n",
      "Embedding AP881230-0051 331/79923...\n",
      "Embedding AP881230-0050 332/79923...\n",
      "Embedding AP881230-0049 333/79923...\n",
      "Embedding AP881230-0048 334/79923...\n",
      "Embedding AP881230-0047 335/79923...\n",
      "Embedding AP881230-0046 336/79923...\n",
      "Embedding AP881230-0045 337/79923...\n",
      "Embedding AP881230-0044 338/79923...\n",
      "Embedding AP881230-0043 339/79923...\n",
      "Embedding AP881230-0042 340/79923...\n",
      "Embedding AP881230-0041 341/79923...\n",
      "Embedding AP881230-0040 342/79923...\n",
      "Embedding AP881230-0039 343/79923...\n",
      "Embedding AP881230-0038 344/79923...\n",
      "Embedding AP881230-0037 345/79923...\n",
      "Embedding AP881230-0036 346/79923...\n",
      "Embedding AP881230-0035 347/79923...\n",
      "Embedding AP881230-0034 348/79923...\n",
      "Embedding AP881230-0033 349/79923...\n",
      "Embedding AP881230-0032 350/79923...\n",
      "Embedding AP881230-0031 351/79923...\n",
      "Embedding AP881230-0030 352/79923...\n",
      "Embedding AP881230-0029 353/79923...\n",
      "Embedding AP881230-0028 354/79923...\n",
      "Embedding AP881230-0027 355/79923...\n",
      "Embedding AP881230-0026 356/79923...\n",
      "Embedding AP881230-0025 357/79923...\n",
      "Embedding AP881230-0024 358/79923...\n",
      "Embedding AP881230-0023 359/79923...\n",
      "Embedding AP881230-0022 360/79923...\n",
      "Embedding AP881230-0021 361/79923...\n",
      "Embedding AP881230-0020 362/79923...\n",
      "Embedding AP881230-0019 363/79923...\n",
      "Embedding AP881230-0018 364/79923...\n",
      "Embedding AP881230-0017 365/79923...\n",
      "Embedding AP881230-0016 366/79923...\n",
      "Embedding AP881230-0015 367/79923...\n",
      "Embedding AP881230-0014 368/79923...\n",
      "Embedding AP881230-0013 369/79923...\n",
      "Embedding AP881230-0012 370/79923...\n",
      "Embedding AP881230-0011 371/79923...\n",
      "Embedding AP881230-0010 372/79923...\n",
      "Embedding AP881230-0009 373/79923...\n",
      "Embedding AP881230-0008 374/79923...\n",
      "Embedding AP881230-0007 375/79923...\n",
      "Embedding AP881230-0006 376/79923...\n",
      "Embedding AP881230-0005 377/79923...\n",
      "Embedding AP881230-0004 378/79923...\n",
      "Embedding AP881230-0003 379/79923...\n",
      "Embedding AP881230-0002 380/79923...\n",
      "Embedding AP881230-0001 381/79923...\n",
      "Embedding AP881229-0208 382/79923...\n",
      "Embedding AP881229-0207 383/79923...\n",
      "Embedding AP881229-0206 384/79923...\n",
      "Embedding AP881229-0205 385/79923...\n",
      "Embedding AP881229-0204 386/79923...\n",
      "Embedding AP881229-0203 387/79923...\n",
      "Embedding AP881229-0202 388/79923...\n",
      "Embedding AP881229-0201 389/79923...\n",
      "Embedding AP881229-0200 390/79923...\n",
      "Embedding AP881229-0199 391/79923...\n",
      "Embedding AP881229-0198 392/79923...\n",
      "Embedding AP881229-0197 393/79923...\n",
      "Embedding AP881229-0196 394/79923...\n",
      "Embedding AP881229-0195 395/79923...\n",
      "Embedding AP881229-0194 396/79923...\n",
      "Embedding AP881229-0193 397/79923...\n",
      "Embedding AP881229-0192 398/79923...\n",
      "Embedding AP881229-0191 399/79923...\n",
      "Embedding AP881229-0190 400/79923...\n",
      "Embedding AP881229-0189 401/79923...\n",
      "Embedding AP881229-0188 402/79923...\n",
      "Embedding AP881229-0187 403/79923...\n",
      "Embedding AP881229-0186 404/79923...\n",
      "Embedding AP881229-0185 405/79923...\n",
      "Embedding AP881229-0184 406/79923...\n",
      "Embedding AP881229-0183 407/79923...\n",
      "Embedding AP881229-0182 408/79923...\n",
      "Embedding AP881229-0181 409/79923...\n",
      "Embedding AP881229-0180 410/79923...\n",
      "Embedding AP881229-0179 411/79923...\n",
      "Embedding AP881229-0178 412/79923...\n",
      "Embedding AP881229-0177 413/79923...\n",
      "Embedding AP881229-0176 414/79923...\n",
      "Embedding AP881229-0175 415/79923...\n",
      "Embedding AP881229-0174 416/79923...\n",
      "Embedding AP881229-0173 417/79923...\n",
      "Embedding AP881229-0172 418/79923...\n",
      "Embedding AP881229-0171 419/79923...\n",
      "Embedding AP881229-0170 420/79923...\n",
      "Embedding AP881229-0169 421/79923...\n",
      "Embedding AP881229-0168 422/79923...\n",
      "Embedding AP881229-0167 423/79923...\n",
      "Embedding AP881229-0166 424/79923...\n",
      "Embedding AP881229-0165 425/79923...\n",
      "Embedding AP881229-0164 426/79923...\n",
      "Embedding AP881229-0163 427/79923...\n",
      "Embedding AP881229-0162 428/79923...\n",
      "Embedding AP881229-0161 429/79923...\n",
      "Embedding AP881229-0160 430/79923...\n",
      "Embedding AP881229-0159 431/79923...\n",
      "Embedding AP881229-0158 432/79923...\n",
      "Embedding AP881229-0157 433/79923...\n",
      "Embedding AP881229-0156 434/79923...\n",
      "Embedding AP881229-0155 435/79923...\n",
      "Embedding AP881229-0154 436/79923...\n",
      "Embedding AP881229-0153 437/79923...\n",
      "Embedding AP881229-0152 438/79923...\n",
      "Embedding AP881229-0151 439/79923...\n",
      "Embedding AP881229-0150 440/79923...\n",
      "Embedding AP881229-0149 441/79923...\n",
      "Embedding AP881229-0148 442/79923...\n",
      "Embedding AP881229-0147 443/79923...\n",
      "Embedding AP881229-0146 444/79923...\n",
      "Embedding AP881229-0145 445/79923...\n",
      "Embedding AP881229-0144 446/79923...\n",
      "Embedding AP881229-0143 447/79923...\n",
      "Embedding AP881229-0142 448/79923...\n",
      "Embedding AP881229-0141 449/79923...\n",
      "Embedding AP881229-0140 450/79923...\n",
      "Embedding AP881229-0139 451/79923...\n",
      "Embedding AP881229-0138 452/79923...\n",
      "Embedding AP881229-0137 453/79923...\n",
      "Embedding AP881229-0136 454/79923...\n",
      "Embedding AP881229-0135 455/79923...\n",
      "Embedding AP881229-0134 456/79923...\n",
      "Embedding AP881229-0133 457/79923...\n",
      "Embedding AP881229-0132 458/79923...\n",
      "Embedding AP881229-0131 459/79923...\n",
      "Embedding AP881229-0130 460/79923...\n",
      "Embedding AP881229-0129 461/79923...\n",
      "Embedding AP881229-0128 462/79923...\n",
      "Embedding AP881229-0127 463/79923...\n",
      "Embedding AP881229-0126 464/79923...\n",
      "Embedding AP881229-0125 465/79923...\n",
      "Embedding AP881229-0124 466/79923...\n",
      "Embedding AP881229-0123 467/79923...\n",
      "Embedding AP881229-0122 468/79923...\n",
      "Embedding AP881229-0121 469/79923...\n",
      "Embedding AP881229-0120 470/79923...\n",
      "Embedding AP881229-0119 471/79923...\n",
      "Embedding AP881229-0118 472/79923...\n",
      "Embedding AP881229-0117 473/79923...\n",
      "Embedding AP881229-0116 474/79923...\n",
      "Embedding AP881229-0115 475/79923...\n",
      "Embedding AP881229-0114 476/79923...\n",
      "Embedding AP881229-0113 477/79923...\n",
      "Embedding AP881229-0112 478/79923...\n",
      "Embedding AP881229-0111 479/79923...\n",
      "Embedding AP881229-0110 480/79923...\n",
      "Embedding AP881229-0109 481/79923...\n",
      "Embedding AP881229-0108 482/79923...\n",
      "Embedding AP881229-0107 483/79923...\n",
      "Embedding AP881229-0106 484/79923...\n",
      "Embedding AP881229-0105 485/79923...\n",
      "Embedding AP881229-0104 486/79923...\n",
      "Embedding AP881229-0103 487/79923...\n",
      "Embedding AP881229-0102 488/79923...\n",
      "Embedding AP881229-0101 489/79923...\n",
      "Embedding AP881229-0100 490/79923...\n",
      "Embedding AP881229-0099 491/79923...\n",
      "Embedding AP881229-0098 492/79923...\n",
      "Embedding AP881229-0097 493/79923...\n",
      "Embedding AP881229-0096 494/79923...\n",
      "Embedding AP881229-0095 495/79923...\n",
      "Embedding AP881229-0094 496/79923...\n",
      "Embedding AP881229-0093 497/79923...\n",
      "Embedding AP881229-0092 498/79923...\n",
      "Embedding AP881229-0091 499/79923...\n",
      "Embedding AP881229-0090 500/79923...\n",
      "Embedding AP881229-0089 501/79923...\n",
      "Embedding AP881229-0088 502/79923...\n",
      "Embedding AP881229-0087 503/79923...\n",
      "Embedding AP881229-0086 504/79923...\n",
      "Embedding AP881229-0085 505/79923...\n",
      "Embedding AP881229-0084 506/79923...\n",
      "Embedding AP881229-0083 507/79923...\n",
      "Embedding AP881229-0082 508/79923...\n",
      "Embedding AP881229-0081 509/79923...\n",
      "Embedding AP881229-0080 510/79923...\n",
      "Embedding AP881229-0079 511/79923...\n",
      "Embedding AP881229-0078 512/79923...\n",
      "Embedding AP881229-0077 513/79923...\n",
      "Embedding AP881229-0076 514/79923...\n",
      "Embedding AP881229-0075 515/79923...\n",
      "Embedding AP881229-0074 516/79923...\n",
      "Embedding AP881229-0073 517/79923...\n",
      "Embedding AP881229-0072 518/79923...\n",
      "Embedding AP881229-0071 519/79923...\n",
      "Embedding AP881229-0070 520/79923...\n",
      "Embedding AP881229-0069 521/79923...\n",
      "Embedding AP881229-0068 522/79923...\n",
      "Embedding AP881229-0067 523/79923...\n",
      "Embedding AP881229-0066 524/79923...\n",
      "Embedding AP881229-0065 525/79923...\n",
      "Embedding AP881229-0064 526/79923...\n",
      "Embedding AP881229-0063 527/79923...\n",
      "Embedding AP881229-0062 528/79923...\n",
      "Embedding AP881229-0061 529/79923...\n",
      "Embedding AP881229-0060 530/79923...\n",
      "Embedding AP881229-0059 531/79923...\n",
      "Embedding AP881229-0058 532/79923...\n",
      "Embedding AP881229-0057 533/79923...\n",
      "Embedding AP881229-0056 534/79923...\n",
      "Embedding AP881229-0055 535/79923...\n",
      "Embedding AP881229-0054 536/79923...\n",
      "Embedding AP881229-0053 537/79923...\n",
      "Embedding AP881229-0052 538/79923...\n",
      "Embedding AP881229-0051 539/79923...\n",
      "Embedding AP881229-0050 540/79923...\n",
      "Embedding AP881229-0049 541/79923...\n",
      "Embedding AP881229-0048 542/79923...\n",
      "Embedding AP881229-0047 543/79923...\n",
      "Embedding AP881229-0046 544/79923...\n",
      "Embedding AP881229-0045 545/79923...\n",
      "Embedding AP881229-0044 546/79923...\n",
      "Embedding AP881229-0043 547/79923...\n",
      "Embedding AP881229-0042 548/79923...\n",
      "Embedding AP881229-0041 549/79923...\n",
      "Embedding AP881229-0040 550/79923...\n",
      "Embedding AP881229-0039 551/79923...\n",
      "Embedding AP881229-0038 552/79923...\n",
      "Embedding AP881229-0037 553/79923...\n",
      "Embedding AP881229-0036 554/79923...\n",
      "Embedding AP881229-0035 555/79923...\n",
      "Embedding AP881229-0034 556/79923...\n",
      "Embedding AP881229-0033 557/79923...\n",
      "Embedding AP881229-0032 558/79923...\n",
      "Embedding AP881229-0031 559/79923...\n",
      "Embedding AP881229-0030 560/79923...\n",
      "Embedding AP881229-0029 561/79923...\n",
      "Embedding AP881229-0028 562/79923...\n",
      "Embedding AP881229-0027 563/79923...\n",
      "Embedding AP881229-0026 564/79923...\n",
      "Embedding AP881229-0025 565/79923...\n",
      "Embedding AP881229-0024 566/79923...\n",
      "Embedding AP881229-0023 567/79923...\n",
      "Embedding AP881229-0022 568/79923...\n",
      "Embedding AP881229-0021 569/79923...\n",
      "Embedding AP881229-0020 570/79923...\n",
      "Embedding AP881229-0019 571/79923...\n",
      "Embedding AP881229-0018 572/79923...\n",
      "Embedding AP881229-0017 573/79923...\n",
      "Embedding AP881229-0016 574/79923...\n",
      "Embedding AP881229-0015 575/79923...\n",
      "Embedding AP881229-0014 576/79923...\n",
      "Embedding AP881229-0013 577/79923...\n",
      "Embedding AP881229-0012 578/79923...\n",
      "Embedding AP881229-0011 579/79923...\n",
      "Embedding AP881229-0010 580/79923...\n",
      "Embedding AP881229-0009 581/79923...\n",
      "Embedding AP881229-0008 582/79923...\n",
      "Embedding AP881229-0007 583/79923...\n",
      "Embedding AP881229-0006 584/79923...\n",
      "Embedding AP881229-0005 585/79923...\n",
      "Embedding AP881229-0004 586/79923...\n",
      "Embedding AP881229-0003 587/79923...\n",
      "Embedding AP881229-0002 588/79923...\n",
      "Embedding AP881229-0001 589/79923...\n",
      "Embedding AP881228-0237 590/79923...\n",
      "Embedding AP881228-0236 591/79923...\n",
      "Embedding AP881228-0235 592/79923...\n",
      "Embedding AP881228-0234 593/79923...\n",
      "Embedding AP881228-0233 594/79923...\n",
      "Embedding AP881228-0232 595/79923...\n",
      "Embedding AP881228-0231 596/79923...\n",
      "Embedding AP881228-0230 597/79923...\n",
      "Embedding AP881228-0229 598/79923...\n",
      "Embedding AP881228-0228 599/79923...\n",
      "Embedding AP881228-0227 600/79923...\n",
      "Embedding AP881228-0226 601/79923...\n",
      "Embedding AP881228-0225 602/79923...\n",
      "Embedding AP881228-0224 603/79923...\n",
      "Embedding AP881228-0223 604/79923...\n",
      "Embedding AP881228-0222 605/79923...\n",
      "Embedding AP881228-0221 606/79923...\n",
      "Embedding AP881228-0220 607/79923...\n",
      "Embedding AP881228-0219 608/79923...\n",
      "Embedding AP881228-0218 609/79923...\n",
      "Embedding AP881228-0217 610/79923...\n",
      "Embedding AP881228-0216 611/79923...\n",
      "Embedding AP881228-0215 612/79923...\n",
      "Embedding AP881228-0214 613/79923...\n",
      "Embedding AP881228-0213 614/79923...\n",
      "Embedding AP881228-0212 615/79923...\n",
      "Embedding AP881228-0211 616/79923...\n",
      "Embedding AP881228-0210 617/79923...\n",
      "Embedding AP881228-0209 618/79923...\n",
      "Embedding AP881228-0208 619/79923...\n",
      "Embedding AP881228-0207 620/79923...\n",
      "Embedding AP881228-0206 621/79923...\n",
      "Embedding AP881228-0205 622/79923...\n",
      "Embedding AP881228-0204 623/79923...\n",
      "Embedding AP881228-0203 624/79923...\n",
      "Embedding AP881228-0202 625/79923...\n",
      "Embedding AP881228-0201 626/79923...\n",
      "Embedding AP881228-0200 627/79923...\n",
      "Embedding AP881228-0199 628/79923...\n",
      "Embedding AP881228-0198 629/79923...\n",
      "Embedding AP881228-0197 630/79923...\n",
      "Embedding AP881228-0196 631/79923...\n",
      "Embedding AP881228-0195 632/79923...\n",
      "Embedding AP881228-0194 633/79923...\n",
      "Embedding AP881228-0193 634/79923...\n",
      "Embedding AP881228-0192 635/79923...\n",
      "Embedding AP881228-0191 636/79923...\n",
      "Embedding AP881228-0190 637/79923...\n",
      "Embedding AP881228-0189 638/79923...\n",
      "Embedding AP881228-0188 639/79923...\n",
      "Embedding AP881228-0187 640/79923...\n",
      "Embedding AP881228-0186 641/79923...\n",
      "Embedding AP881228-0185 642/79923...\n",
      "Embedding AP881228-0184 643/79923...\n",
      "Embedding AP881228-0183 644/79923...\n",
      "Embedding AP881228-0182 645/79923...\n",
      "Embedding AP881228-0181 646/79923...\n",
      "Embedding AP881228-0180 647/79923...\n",
      "Embedding AP881228-0179 648/79923...\n",
      "Embedding AP881228-0178 649/79923...\n",
      "Embedding AP881228-0177 650/79923...\n",
      "Embedding AP881228-0176 651/79923...\n",
      "Embedding AP881228-0175 652/79923...\n",
      "Embedding AP881228-0174 653/79923...\n",
      "Embedding AP881228-0173 654/79923...\n",
      "Embedding AP881228-0172 655/79923...\n",
      "Embedding AP881228-0171 656/79923...\n",
      "Embedding AP881228-0170 657/79923...\n",
      "Embedding AP881228-0169 658/79923...\n",
      "Embedding AP881228-0168 659/79923...\n",
      "Embedding AP881228-0167 660/79923...\n",
      "Embedding AP881228-0166 661/79923...\n",
      "Embedding AP881228-0165 662/79923...\n",
      "Embedding AP881228-0164 663/79923...\n",
      "Embedding AP881228-0163 664/79923...\n",
      "Embedding AP881228-0162 665/79923...\n",
      "Embedding AP881228-0161 666/79923...\n",
      "Embedding AP881228-0160 667/79923...\n",
      "Embedding AP881228-0159 668/79923...\n",
      "Embedding AP881228-0158 669/79923...\n",
      "Embedding AP881228-0157 670/79923...\n",
      "Embedding AP881228-0156 671/79923...\n",
      "Embedding AP881228-0155 672/79923...\n",
      "Embedding AP881228-0154 673/79923...\n",
      "Embedding AP881228-0153 674/79923...\n",
      "Embedding AP881228-0152 675/79923...\n",
      "Embedding AP881228-0151 676/79923...\n",
      "Embedding AP881228-0150 677/79923...\n",
      "Embedding AP881228-0149 678/79923...\n",
      "Embedding AP881228-0148 679/79923...\n",
      "Embedding AP881228-0147 680/79923...\n",
      "Embedding AP881228-0146 681/79923...\n",
      "Embedding AP881228-0145 682/79923...\n",
      "Embedding AP881228-0144 683/79923...\n",
      "Embedding AP881228-0143 684/79923...\n",
      "Embedding AP881228-0142 685/79923...\n",
      "Embedding AP881228-0141 686/79923...\n",
      "Embedding AP881228-0140 687/79923...\n",
      "Embedding AP881228-0139 688/79923...\n",
      "Embedding AP881228-0138 689/79923...\n",
      "Embedding AP881228-0137 690/79923...\n",
      "Embedding AP881228-0136 691/79923...\n",
      "Embedding AP881228-0135 692/79923...\n",
      "Embedding AP881228-0134 693/79923...\n",
      "Embedding AP881228-0133 694/79923...\n",
      "Embedding AP881228-0132 695/79923...\n",
      "Embedding AP881228-0131 696/79923...\n",
      "Embedding AP881228-0130 697/79923...\n",
      "Embedding AP881228-0129 698/79923...\n",
      "Embedding AP881228-0128 699/79923...\n",
      "Embedding AP881228-0127 700/79923...\n",
      "Embedding AP881228-0126 701/79923...\n",
      "Embedding AP881228-0125 702/79923...\n",
      "Embedding AP881228-0124 703/79923...\n",
      "Embedding AP881228-0123 704/79923...\n",
      "Embedding AP881228-0122 705/79923...\n",
      "Embedding AP881228-0121 706/79923...\n",
      "Embedding AP881228-0120 707/79923...\n",
      "Embedding AP881228-0119 708/79923...\n",
      "Embedding AP881228-0118 709/79923...\n",
      "Embedding AP881228-0117 710/79923...\n",
      "Embedding AP881228-0116 711/79923...\n",
      "Embedding AP881228-0115 712/79923...\n",
      "Embedding AP881228-0114 713/79923...\n",
      "Embedding AP881228-0113 714/79923...\n",
      "Embedding AP881228-0112 715/79923...\n",
      "Embedding AP881228-0111 716/79923...\n",
      "Embedding AP881228-0110 717/79923...\n",
      "Embedding AP881228-0109 718/79923...\n",
      "Embedding AP881228-0108 719/79923...\n",
      "Embedding AP881228-0107 720/79923...\n",
      "Embedding AP881228-0106 721/79923...\n",
      "Embedding AP881228-0105 722/79923...\n",
      "Embedding AP881228-0104 723/79923...\n",
      "Embedding AP881228-0103 724/79923...\n",
      "Embedding AP881228-0102 725/79923...\n",
      "Embedding AP881228-0101 726/79923...\n",
      "Embedding AP881228-0100 727/79923...\n",
      "Embedding AP881228-0099 728/79923...\n",
      "Embedding AP881228-0098 729/79923...\n",
      "Embedding AP881228-0097 730/79923...\n",
      "Embedding AP881228-0096 731/79923...\n",
      "Embedding AP881228-0095 732/79923...\n",
      "Embedding AP881228-0094 733/79923...\n",
      "Embedding AP881228-0093 734/79923...\n",
      "Embedding AP881228-0092 735/79923...\n",
      "Embedding AP881228-0091 736/79923...\n",
      "Embedding AP881228-0090 737/79923...\n",
      "Embedding AP881228-0089 738/79923...\n",
      "Embedding AP881228-0088 739/79923...\n",
      "Embedding AP881228-0087 740/79923...\n",
      "Embedding AP881228-0086 741/79923...\n",
      "Embedding AP881228-0085 742/79923...\n",
      "Embedding AP881228-0084 743/79923...\n",
      "Embedding AP881228-0083 744/79923...\n",
      "Embedding AP881228-0082 745/79923...\n",
      "Embedding AP881228-0081 746/79923...\n",
      "Embedding AP881228-0080 747/79923...\n",
      "Embedding AP881228-0079 748/79923...\n",
      "Embedding AP881228-0078 749/79923...\n",
      "Embedding AP881228-0077 750/79923...\n",
      "Embedding AP881228-0076 751/79923...\n",
      "Embedding AP881228-0075 752/79923...\n",
      "Embedding AP881228-0074 753/79923...\n",
      "Embedding AP881228-0073 754/79923...\n",
      "Embedding AP881228-0072 755/79923...\n",
      "Embedding AP881228-0071 756/79923...\n",
      "Embedding AP881228-0070 757/79923...\n",
      "Embedding AP881228-0069 758/79923...\n",
      "Embedding AP881228-0068 759/79923...\n",
      "Embedding AP881228-0067 760/79923...\n",
      "Embedding AP881228-0066 761/79923...\n",
      "Embedding AP881228-0065 762/79923...\n",
      "Embedding AP881228-0064 763/79923...\n",
      "Embedding AP881228-0063 764/79923...\n",
      "Embedding AP881228-0062 765/79923...\n",
      "Embedding AP881228-0061 766/79923...\n",
      "Embedding AP881228-0060 767/79923...\n",
      "Embedding AP881228-0059 768/79923...\n",
      "Embedding AP881228-0058 769/79923...\n",
      "Embedding AP881228-0057 770/79923...\n",
      "Embedding AP881228-0056 771/79923...\n",
      "Embedding AP881228-0055 772/79923...\n",
      "Embedding AP881228-0054 773/79923...\n",
      "Embedding AP881228-0053 774/79923...\n",
      "Embedding AP881228-0052 775/79923...\n",
      "Embedding AP881228-0051 776/79923...\n",
      "Embedding AP881228-0050 777/79923...\n",
      "Embedding AP881228-0049 778/79923...\n",
      "Embedding AP881228-0048 779/79923...\n",
      "Embedding AP881228-0047 780/79923...\n",
      "Embedding AP881228-0046 781/79923...\n",
      "Embedding AP881228-0045 782/79923...\n",
      "Embedding AP881228-0044 783/79923...\n",
      "Embedding AP881228-0043 784/79923...\n",
      "Embedding AP881228-0042 785/79923...\n",
      "Embedding AP881228-0041 786/79923...\n",
      "Embedding AP881228-0040 787/79923...\n",
      "Embedding AP881228-0039 788/79923...\n",
      "Embedding AP881228-0038 789/79923...\n",
      "Embedding AP881228-0037 790/79923...\n",
      "Embedding AP881228-0036 791/79923...\n",
      "Embedding AP881228-0035 792/79923...\n",
      "Embedding AP881228-0034 793/79923...\n",
      "Embedding AP881228-0033 794/79923...\n",
      "Embedding AP881228-0032 795/79923...\n",
      "Embedding AP881228-0031 796/79923...\n",
      "Embedding AP881228-0030 797/79923...\n",
      "Embedding AP881228-0029 798/79923...\n",
      "Embedding AP881228-0028 799/79923...\n",
      "Embedding AP881228-0027 800/79923...\n",
      "Embedding AP881228-0026 801/79923...\n",
      "Embedding AP881228-0025 802/79923...\n",
      "Embedding AP881228-0024 803/79923...\n",
      "Embedding AP881228-0023 804/79923...\n",
      "Embedding AP881228-0022 805/79923...\n",
      "Embedding AP881228-0021 806/79923...\n",
      "Embedding AP881228-0020 807/79923...\n",
      "Embedding AP881228-0019 808/79923...\n",
      "Embedding AP881228-0018 809/79923...\n",
      "Embedding AP881228-0017 810/79923...\n",
      "Embedding AP881228-0016 811/79923...\n",
      "Embedding AP881228-0015 812/79923...\n",
      "Embedding AP881228-0014 813/79923...\n",
      "Embedding AP881228-0013 814/79923...\n",
      "Embedding AP881228-0012 815/79923...\n",
      "Embedding AP881228-0011 816/79923...\n",
      "Embedding AP881228-0010 817/79923...\n",
      "Embedding AP881228-0009 818/79923...\n",
      "Embedding AP881228-0008 819/79923...\n",
      "Embedding AP881228-0007 820/79923...\n",
      "Embedding AP881228-0006 821/79923...\n",
      "Embedding AP881228-0005 822/79923...\n",
      "Embedding AP881228-0004 823/79923...\n",
      "Embedding AP881228-0003 824/79923...\n",
      "Embedding AP881228-0002 825/79923...\n",
      "Embedding AP881228-0001 826/79923...\n",
      "Embedding AP881227-0225 827/79923...\n",
      "Embedding AP881227-0224 828/79923...\n",
      "Embedding AP881227-0223 829/79923...\n",
      "Embedding AP881227-0222 830/79923...\n",
      "Embedding AP881227-0221 831/79923...\n",
      "Embedding AP881227-0220 832/79923...\n",
      "Embedding AP881227-0219 833/79923...\n",
      "Embedding AP881227-0218 834/79923...\n",
      "Embedding AP881227-0217 835/79923...\n",
      "Embedding AP881227-0216 836/79923...\n",
      "Embedding AP881227-0215 837/79923...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# Calculate embedding for each document\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmbedding \u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mdoc_no\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(preprocessed_documents)\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m doc_embed \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(doc\u001b[39m.\u001b[39;49mdoc_text, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# write the document embedding to a file\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39membedding_saves/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mdoc_no\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[0;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:74\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[0;32m     72\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 74\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     75\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[0;32m     77\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:621\u001b[0m, in \u001b[0;36mGPTNeoModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    613\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    614\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    615\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m         head_mask[i],\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    620\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[0;32m    623\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    624\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    625\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    626\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    627\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    628\u001b[0m     )\n\u001b[0;32m    630\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:327\u001b[0m, in \u001b[0;36mGPTNeoBlock.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    325\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    326\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 327\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    328\u001b[0m     hidden_states,\n\u001b[0;32m    329\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    330\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    331\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    332\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    333\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    334\u001b[0m )\n\u001b[0;32m    335\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    336\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:279\u001b[0m, in \u001b[0;36mGPTNeoAttention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    271\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    272\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    278\u001b[0m ):\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    280\u001b[0m         hidden_states,\n\u001b[0;32m    281\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    282\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    283\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    284\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    285\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    286\u001b[0m     )\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:225\u001b[0m, in \u001b[0;36mGPTNeoSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    223\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(hidden_states)\n\u001b[0;32m    224\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m--> 225\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(hidden_states)\n\u001b[0;32m    227\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    228\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_heads(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Compute the embeddings\n",
    "for x, doc in enumerate(reversed(preprocessed_documents)):\n",
    "  # Clear the cache\n",
    "  torch.cuda.empty_cache()\n",
    "  # skip the document if it exists, otherwise compute it\n",
    "  if os.path.exists(f'embedding_saves/{model_name}/{doc.doc_no.strip()}.pickle'):\n",
    "    continue\n",
    "  else:\n",
    "    os.makedirs(f'embedding_saves/{model_name}', exist_ok=True)\n",
    "    # Calculate embedding for each document\n",
    "    print(f'Embedding {doc.doc_no.strip()} {x}/{len(preprocessed_documents)}...')\n",
    "    doc_embed = model.encode(doc.doc_text, show_progress_bar=False)\n",
    "    # write the document embedding to a file\n",
    "    with open(f'embedding_saves/{model_name}/{doc.doc_no.strip()}.pickle', 'wb') as f:\n",
    "      pickle.dump(doc_embed, f)\n",
    "print(f'Finished computing embeddings for {model_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import gzip\n",
    "# import os\n",
    "\n",
    "# # assuming you have a list of Document objects called documents\n",
    "# # and assuming you have already populated the vector attribute of each Document object\n",
    "\n",
    "# # define the headers for your CSV file\n",
    "# headers = ['doc_no', 'vector']\n",
    "\n",
    "# # open the CSV file in 'w' mode and write the headers\n",
    "# with open(f\"embedding_saves/{model_name}.csv\", mode='w', newline='') as file:\n",
    "#   writer = csv.writer(file)\n",
    "#   writer.writerow(headers)\n",
    "\n",
    "#   # loop through each Document object and write its attributes to the CSV file\n",
    "#   for x, document in enumerate(preprocessed_documents):\n",
    "#     writer.writerow([document.doc_no, doc_embeddings[x]])\n",
    "\n",
    "# # gzip the CSV file\n",
    "# with open(f\"embedding_saves/{model_name}.csv\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.csv.gz\", 'wb') as f_out:\n",
    "#     f_out.writelines(f_in)\n",
    "\n",
    "# os.remove(f\"embedding_saves/{model_name}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/79923\n",
      "1000/79923\n",
      "2000/79923\n",
      "3000/79923\n",
      "4000/79923\n",
      "5000/79923\n",
      "6000/79923\n",
      "7000/79923\n",
      "8000/79923\n",
      "9000/79923\n",
      "10000/79923\n",
      "11000/79923\n",
      "12000/79923\n",
      "13000/79923\n",
      "14000/79923\n",
      "15000/79923\n",
      "16000/79923\n",
      "17000/79923\n",
      "18000/79923\n",
      "19000/79923\n",
      "20000/79923\n",
      "21000/79923\n",
      "22000/79923\n",
      "23000/79923\n",
      "24000/79923\n",
      "25000/79923\n",
      "26000/79923\n",
      "27000/79923\n",
      "28000/79923\n",
      "29000/79923\n",
      "30000/79923\n",
      "31000/79923\n",
      "32000/79923\n",
      "33000/79923\n",
      "34000/79923\n",
      "35000/79923\n",
      "36000/79923\n",
      "37000/79923\n",
      "38000/79923\n",
      "39000/79923\n",
      "40000/79923\n",
      "41000/79923\n",
      "42000/79923\n",
      "43000/79923\n",
      "44000/79923\n",
      "45000/79923\n",
      "46000/79923\n",
      "47000/79923\n",
      "48000/79923\n",
      "49000/79923\n",
      "50000/79923\n",
      "51000/79923\n",
      "52000/79923\n",
      "53000/79923\n",
      "54000/79923\n",
      "55000/79923\n",
      "56000/79923\n",
      "57000/79923\n",
      "58000/79923\n",
      "59000/79923\n",
      "60000/79923\n",
      "61000/79923\n",
      "62000/79923\n",
      "63000/79923\n",
      "64000/79923\n",
      "65000/79923\n",
      "66000/79923\n",
      "67000/79923\n",
      "68000/79923\n",
      "69000/79923\n",
      "70000/79923\n",
      "71000/79923\n",
      "72000/79923\n",
      "73000/79923\n",
      "74000/79923\n",
      "75000/79923\n",
      "76000/79923\n",
      "77000/79923\n",
      "78000/79923\n",
      "79000/79923\n"
     ]
    }
   ],
   "source": [
    "# Read all the embeddings from the files in the directory\n",
    "doc_embeddings = []\n",
    "for x, doc in enumerate(preprocessed_documents):\n",
    "  if x % 1000 == 0:\n",
    "    print(f'{x}/{len(preprocessed_documents)}')\n",
    "  filename = doc.doc_no.strip()\n",
    "  if os.path.exists(f'embedding_saves/{model_name}/{filename}.pickle'):\n",
    "    # print(f'Loading embedding for {model_name}/{filename} {x}/{len(preprocessed_documents)}')\n",
    "    with open(f'embedding_saves/{model_name}/{filename}.pickle', 'rb') as f:\n",
    "      doc_embeddings.append(pickle.load(f))\n",
    "  else:\n",
    "    print(f'Embedding for {model_name}/{filename} doesn\\'t exist {x}/{len(preprocessed_documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Save the embeddings\n",
    "doc_embeddings = np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings computed, saving to file: embedding_saves/gtr-t5-xxl.pickle.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "print(f'Embeddings computed, saving to file: embedding_saves/{model_name}.pickle.gz')\n",
    "# store the embeddings in a pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'wb') as f:\n",
    "  pickle.dump(np.array(doc_embeddings), f)\n",
    "# gzip the pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.pickle.gz\", 'wb') as f_out:\n",
    "  f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all the documents and search for the top 1000 results\n",
    "def query_retrieve(model, preprocessed_documents, doc_embeddings, descriptions=False, runid='runid', filename='Results.txt', top_k=1000):\n",
    "  # Extract the topics\n",
    "  topics = extract_topics('topics1-50.txt', descriptions)\n",
    "\n",
    "  file_out = open(filename, 'w')\n",
    "\n",
    "  for i, topic in enumerate(topics):\n",
    "    print(f'Querying for topic {i+1}...')\n",
    "    # Search for the documents\n",
    "    results = search(i, topic['title'], model, preprocessed_documents, doc_embeddings, top_k)\n",
    "    for j, (doc_id, distance) in enumerate(results):\n",
    "      file_out.write(f'{i+1} Q0 {doc_id.strip()} {j+1} {1-distance} {runid}\\n')\n",
    "  file_out.close()\n",
    "  print('Written results to file ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for topic 1...\n",
      "Querying for topic 2...\n",
      "Querying for topic 3...\n",
      "Querying for topic 4...\n",
      "Querying for topic 5...\n",
      "Querying for topic 6...\n",
      "Querying for topic 7...\n",
      "Querying for topic 8...\n",
      "Querying for topic 9...\n",
      "Querying for topic 10...\n",
      "Querying for topic 11...\n",
      "Querying for topic 12...\n",
      "Querying for topic 13...\n",
      "Querying for topic 14...\n",
      "Querying for topic 15...\n",
      "Querying for topic 16...\n",
      "Querying for topic 17...\n",
      "Querying for topic 18...\n",
      "Querying for topic 19...\n",
      "Querying for topic 20...\n",
      "Querying for topic 21...\n",
      "Querying for topic 22...\n",
      "Querying for topic 23...\n",
      "Querying for topic 24...\n",
      "Querying for topic 25...\n",
      "Querying for topic 26...\n",
      "Querying for topic 27...\n",
      "Querying for topic 28...\n",
      "Querying for topic 29...\n",
      "Querying for topic 30...\n",
      "Querying for topic 31...\n",
      "Querying for topic 32...\n",
      "Querying for topic 33...\n",
      "Querying for topic 34...\n",
      "Querying for topic 35...\n",
      "Querying for topic 36...\n",
      "Querying for topic 37...\n",
      "Querying for topic 38...\n",
      "Querying for topic 39...\n",
      "Querying for topic 40...\n",
      "Querying for topic 41...\n",
      "Querying for topic 42...\n",
      "Querying for topic 43...\n",
      "Querying for topic 44...\n",
      "Querying for topic 45...\n",
      "Querying for topic 46...\n",
      "Querying for topic 47...\n",
      "Querying for topic 48...\n",
      "Querying for topic 49...\n",
      "Querying for topic 50...\n",
      "Written results to file  Results-gtr-t5-xxl.txt\n"
     ]
    }
   ],
   "source": [
    "query_retrieve(model, preprocessed_documents, doc_embeddings, descriptions=False, runid='runid', filename=f'Results-{model_name}.txt', top_k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\trunid\n",
      "num_q                 \tall\t50\n",
      "num_ret               \tall\t50000\n",
      "num_rel               \tall\t2099\n",
      "num_rel_ret           \tall\t1292\n",
      "map                   \tall\t0.2369\n",
      "gm_map                \tall\t0.1109\n",
      "Rprec                 \tall\t0.2600\n",
      "bpref                 \tall\t0.3301\n",
      "recip_rank            \tall\t0.6152\n",
      "iprec_at_recall_0.00  \tall\t0.6462\n",
      "iprec_at_recall_0.10  \tall\t0.4958\n",
      "iprec_at_recall_0.20  \tall\t0.4159\n",
      "iprec_at_recall_0.30  \tall\t0.3747\n",
      "iprec_at_recall_0.40  \tall\t0.2919\n",
      "iprec_at_recall_0.50  \tall\t0.2044\n",
      "iprec_at_recall_0.60  \tall\t0.1395\n",
      "iprec_at_recall_0.70  \tall\t0.1032\n",
      "iprec_at_recall_0.80  \tall\t0.0727\n",
      "iprec_at_recall_0.90  \tall\t0.0458\n",
      "iprec_at_recall_1.00  \tall\t0.0242\n",
      "P_5                   \tall\t0.4360\n",
      "P_10                  \tall\t0.3820\n",
      "P_15                  \tall\t0.3467\n",
      "P_20                  \tall\t0.3180\n",
      "P_30                  \tall\t0.2707\n",
      "P_100                 \tall\t0.1442\n",
      "P_200                 \tall\t0.0887\n",
      "P_500                 \tall\t0.0443\n",
      "P_1000                \tall\t0.0258\n"
     ]
    }
   ],
   "source": [
    "!Powershell.exe -Command \".\\trec_eval qrels1-50ap.txt results/Results-SGPT-1.3B-weightedmean-msmarco-specb-bitfit.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard/.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42b34bead2ece13e82fdd57b33000433053e25f8e38f52c0f662c8d14f00a960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
