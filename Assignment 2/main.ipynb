{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "model_name='gtr-t5-xxl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and classes for preprocessing the data\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "  def __init__(self, doc_no, doc_text, tokens):\n",
    "    self.doc_no = doc_no\n",
    "    self.doc_text = doc_text\n",
    "    self.tokens = tokens\n",
    "\n",
    "  def __str__(self):\n",
    "    return 'Document Number: ' + self.doc_no + '\\nDocument Text: ' + self.doc_text + '\\nTokens: ' + str(self.tokens) + '\\n'\n",
    "\n",
    "  def to_dict(self):\n",
    "    return {'docno': self.doc_no, 'doctext': self.doc_text, 'tokens': self.tokens, 'text': ' '.join(self.tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to perform preprocessing on the text\n",
    "def preprocess(file):\n",
    "  with open(file, \"r\") as f:\n",
    "    content = f.read()\n",
    "  documents = re.findall(r'<DOC>(.*?)</DOC>', content, re.DOTALL)\n",
    "  preprocessed_documents = []\n",
    "  for document in documents:\n",
    "    # Get the document number and text\n",
    "    raw_no = re.search(r'<DOCNO>(.*?)</DOCNO>', document, re.DOTALL)\n",
    "    doc_no = raw_no.group(1) if raw_no else ''\n",
    "    raw_text = re.search(r'<TEXT>(.*?)</TEXT>', document, re.DOTALL)\n",
    "    doc_text = raw_text.string if raw_text else ''\n",
    "\n",
    "    # create a document object\n",
    "    doc = Document(doc_no, doc_text, [])\n",
    "    preprocessed_documents.append(doc)\n",
    "  return preprocessed_documents\n",
    "\n",
    "# main function to preprocess a directory of text files\n",
    "def preprocess_directory(directory, num_files=-1):\n",
    "  preprocessed_documents = []\n",
    "  ctr = 0\n",
    "  for filename in os.listdir(directory):\n",
    "    print('Preprocessing file: ', filename)\n",
    "    file = os.path.join(directory, filename)\n",
    "    preprocessed_documents.extend(preprocess(file))\n",
    "    ctr += 1\n",
    "    if ctr == num_files and num_files != -1:\n",
    "      break\n",
    "    \n",
    "  print('preprocessed ', ctr, ' files')\n",
    "  return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file:  AP880212\n",
      "Preprocessing file:  AP880213\n",
      "Preprocessing file:  AP880214\n",
      "Preprocessing file:  AP880215\n",
      "Preprocessing file:  AP880216\n",
      "Preprocessing file:  AP880217\n",
      "Preprocessing file:  AP880218\n",
      "Preprocessing file:  AP880219\n",
      "Preprocessing file:  AP880220\n",
      "Preprocessing file:  AP880221\n",
      "Preprocessing file:  AP880222\n",
      "Preprocessing file:  AP880223\n",
      "Preprocessing file:  AP880224\n",
      "Preprocessing file:  AP880225\n",
      "Preprocessing file:  AP880226\n",
      "Preprocessing file:  AP880227\n",
      "Preprocessing file:  AP880228\n",
      "Preprocessing file:  AP880229\n",
      "Preprocessing file:  AP880301\n",
      "Preprocessing file:  AP880302\n",
      "Preprocessing file:  AP880303\n",
      "Preprocessing file:  AP880304\n",
      "Preprocessing file:  AP880307\n",
      "Preprocessing file:  AP880308\n",
      "Preprocessing file:  AP880309\n",
      "Preprocessing file:  AP880310\n",
      "Preprocessing file:  AP880311\n",
      "Preprocessing file:  AP880312\n",
      "Preprocessing file:  AP880313\n",
      "Preprocessing file:  AP880314\n",
      "Preprocessing file:  AP880315\n",
      "Preprocessing file:  AP880316\n",
      "Preprocessing file:  AP880317\n",
      "Preprocessing file:  AP880318\n",
      "Preprocessing file:  AP880319\n",
      "Preprocessing file:  AP880320\n",
      "Preprocessing file:  AP880321\n",
      "Preprocessing file:  AP880322\n",
      "Preprocessing file:  AP880323\n",
      "Preprocessing file:  AP880324\n",
      "Preprocessing file:  AP880325\n",
      "Preprocessing file:  AP880326\n",
      "Preprocessing file:  AP880327\n",
      "Preprocessing file:  AP880328\n",
      "Preprocessing file:  AP880329\n",
      "Preprocessing file:  AP880330\n",
      "Preprocessing file:  AP880331\n",
      "Preprocessing file:  AP880401\n",
      "Preprocessing file:  AP880402\n",
      "Preprocessing file:  AP880403\n",
      "Preprocessing file:  AP880404\n",
      "Preprocessing file:  AP880405\n",
      "Preprocessing file:  AP880406\n",
      "Preprocessing file:  AP880407\n",
      "Preprocessing file:  AP880408\n",
      "Preprocessing file:  AP880409\n",
      "Preprocessing file:  AP880410\n",
      "Preprocessing file:  AP880411\n",
      "Preprocessing file:  AP880412\n",
      "Preprocessing file:  AP880413\n",
      "Preprocessing file:  AP880414\n",
      "Preprocessing file:  AP880415\n",
      "Preprocessing file:  AP880416\n",
      "Preprocessing file:  AP880417\n",
      "Preprocessing file:  AP880418\n",
      "Preprocessing file:  AP880419\n",
      "Preprocessing file:  AP880420\n",
      "Preprocessing file:  AP880421\n",
      "Preprocessing file:  AP880422\n",
      "Preprocessing file:  AP880423\n",
      "Preprocessing file:  AP880424\n",
      "Preprocessing file:  AP880425\n",
      "Preprocessing file:  AP880426\n",
      "Preprocessing file:  AP880427\n",
      "Preprocessing file:  AP880428\n",
      "Preprocessing file:  AP880429\n",
      "Preprocessing file:  AP880430\n",
      "Preprocessing file:  AP880501\n",
      "Preprocessing file:  AP880502\n",
      "Preprocessing file:  AP880503\n",
      "Preprocessing file:  AP880504\n",
      "Preprocessing file:  AP880505\n",
      "Preprocessing file:  AP880506\n",
      "Preprocessing file:  AP880507\n",
      "Preprocessing file:  AP880508\n",
      "Preprocessing file:  AP880509\n",
      "Preprocessing file:  AP880510\n",
      "Preprocessing file:  AP880511\n",
      "Preprocessing file:  AP880512\n",
      "Preprocessing file:  AP880513\n",
      "Preprocessing file:  AP880514\n",
      "Preprocessing file:  AP880515\n",
      "Preprocessing file:  AP880516\n",
      "Preprocessing file:  AP880517\n",
      "Preprocessing file:  AP880518\n",
      "Preprocessing file:  AP880519\n",
      "Preprocessing file:  AP880520\n",
      "Preprocessing file:  AP880521\n",
      "Preprocessing file:  AP880522\n",
      "Preprocessing file:  AP880523\n",
      "Preprocessing file:  AP880524\n",
      "Preprocessing file:  AP880525\n",
      "Preprocessing file:  AP880526\n",
      "Preprocessing file:  AP880527\n",
      "Preprocessing file:  AP880528\n",
      "Preprocessing file:  AP880529\n",
      "Preprocessing file:  AP880530\n",
      "Preprocessing file:  AP880531\n",
      "Preprocessing file:  AP880601\n",
      "Preprocessing file:  AP880602\n",
      "Preprocessing file:  AP880603\n",
      "Preprocessing file:  AP880604\n",
      "Preprocessing file:  AP880605\n",
      "Preprocessing file:  AP880606\n",
      "Preprocessing file:  AP880607\n",
      "Preprocessing file:  AP880608\n",
      "Preprocessing file:  AP880609\n",
      "Preprocessing file:  AP880610\n",
      "Preprocessing file:  AP880611\n",
      "Preprocessing file:  AP880612\n",
      "Preprocessing file:  AP880613\n",
      "Preprocessing file:  AP880614\n",
      "Preprocessing file:  AP880615\n",
      "Preprocessing file:  AP880616\n",
      "Preprocessing file:  AP880617\n",
      "Preprocessing file:  AP880618\n",
      "Preprocessing file:  AP880619\n",
      "Preprocessing file:  AP880620\n",
      "Preprocessing file:  AP880621\n",
      "Preprocessing file:  AP880622\n",
      "Preprocessing file:  AP880623\n",
      "Preprocessing file:  AP880624\n",
      "Preprocessing file:  AP880625\n",
      "Preprocessing file:  AP880626\n",
      "Preprocessing file:  AP880627\n",
      "Preprocessing file:  AP880628\n",
      "Preprocessing file:  AP880629\n",
      "Preprocessing file:  AP880630\n",
      "Preprocessing file:  AP880701\n",
      "Preprocessing file:  AP880702\n",
      "Preprocessing file:  AP880703\n",
      "Preprocessing file:  AP880704\n",
      "Preprocessing file:  AP880705\n",
      "Preprocessing file:  AP880706\n",
      "Preprocessing file:  AP880707\n",
      "Preprocessing file:  AP880708\n",
      "Preprocessing file:  AP880709\n",
      "Preprocessing file:  AP880710\n",
      "Preprocessing file:  AP880711\n",
      "Preprocessing file:  AP880712\n",
      "Preprocessing file:  AP880713\n",
      "Preprocessing file:  AP880714\n",
      "Preprocessing file:  AP880715\n",
      "Preprocessing file:  AP880716\n",
      "Preprocessing file:  AP880717\n",
      "Preprocessing file:  AP880718\n",
      "Preprocessing file:  AP880719\n",
      "Preprocessing file:  AP880720\n",
      "Preprocessing file:  AP880721\n",
      "Preprocessing file:  AP880722\n",
      "Preprocessing file:  AP880723\n",
      "Preprocessing file:  AP880724\n",
      "Preprocessing file:  AP880725\n",
      "Preprocessing file:  AP880726\n",
      "Preprocessing file:  AP880727\n",
      "Preprocessing file:  AP880728\n",
      "Preprocessing file:  AP880729\n",
      "Preprocessing file:  AP880730\n",
      "Preprocessing file:  AP880731\n",
      "Preprocessing file:  AP880801\n",
      "Preprocessing file:  AP880802\n",
      "Preprocessing file:  AP880803\n",
      "Preprocessing file:  AP880804\n",
      "Preprocessing file:  AP880805\n",
      "Preprocessing file:  AP880806\n",
      "Preprocessing file:  AP880807\n",
      "Preprocessing file:  AP880808\n",
      "Preprocessing file:  AP880809\n",
      "Preprocessing file:  AP880810\n",
      "Preprocessing file:  AP880811\n",
      "Preprocessing file:  AP880812\n",
      "Preprocessing file:  AP880813\n",
      "Preprocessing file:  AP880814\n",
      "Preprocessing file:  AP880815\n",
      "Preprocessing file:  AP880816\n",
      "Preprocessing file:  AP880817\n",
      "Preprocessing file:  AP880818\n",
      "Preprocessing file:  AP880819\n",
      "Preprocessing file:  AP880820\n",
      "Preprocessing file:  AP880821\n",
      "Preprocessing file:  AP880822\n",
      "Preprocessing file:  AP880823\n",
      "Preprocessing file:  AP880824\n",
      "Preprocessing file:  AP880825\n",
      "Preprocessing file:  AP880826\n",
      "Preprocessing file:  AP880827\n",
      "Preprocessing file:  AP880828\n",
      "Preprocessing file:  AP880829\n",
      "Preprocessing file:  AP880830\n",
      "Preprocessing file:  AP880831\n",
      "Preprocessing file:  AP880901\n",
      "Preprocessing file:  AP880902\n",
      "Preprocessing file:  AP880903\n",
      "Preprocessing file:  AP880904\n",
      "Preprocessing file:  AP880905\n",
      "Preprocessing file:  AP880906\n",
      "Preprocessing file:  AP880907\n",
      "Preprocessing file:  AP880908\n",
      "Preprocessing file:  AP880909\n",
      "Preprocessing file:  AP880910\n",
      "Preprocessing file:  AP880911\n",
      "Preprocessing file:  AP880912\n",
      "Preprocessing file:  AP880913\n",
      "Preprocessing file:  AP880914\n",
      "Preprocessing file:  AP880915\n",
      "Preprocessing file:  AP880916\n",
      "Preprocessing file:  AP880917\n",
      "Preprocessing file:  AP880918\n",
      "Preprocessing file:  AP880919\n",
      "Preprocessing file:  AP880920\n",
      "Preprocessing file:  AP880921\n",
      "Preprocessing file:  AP880922\n",
      "Preprocessing file:  AP880923\n",
      "Preprocessing file:  AP880924\n",
      "Preprocessing file:  AP880925\n",
      "Preprocessing file:  AP880926\n",
      "Preprocessing file:  AP880927\n",
      "Preprocessing file:  AP880928\n",
      "Preprocessing file:  AP880929\n",
      "Preprocessing file:  AP880930\n",
      "Preprocessing file:  AP881001\n",
      "Preprocessing file:  AP881002\n",
      "Preprocessing file:  AP881003\n",
      "Preprocessing file:  AP881004\n",
      "Preprocessing file:  AP881005\n",
      "Preprocessing file:  AP881006\n",
      "Preprocessing file:  AP881007\n",
      "Preprocessing file:  AP881008\n",
      "Preprocessing file:  AP881009\n",
      "Preprocessing file:  AP881010\n",
      "Preprocessing file:  AP881011\n",
      "Preprocessing file:  AP881012\n",
      "Preprocessing file:  AP881013\n",
      "Preprocessing file:  AP881014\n",
      "Preprocessing file:  AP881015\n",
      "Preprocessing file:  AP881016\n",
      "Preprocessing file:  AP881017\n",
      "Preprocessing file:  AP881018\n",
      "Preprocessing file:  AP881019\n",
      "Preprocessing file:  AP881020\n",
      "Preprocessing file:  AP881021\n",
      "Preprocessing file:  AP881022\n",
      "Preprocessing file:  AP881023\n",
      "Preprocessing file:  AP881024\n",
      "Preprocessing file:  AP881025\n",
      "Preprocessing file:  AP881026\n",
      "Preprocessing file:  AP881027\n",
      "Preprocessing file:  AP881028\n",
      "Preprocessing file:  AP881029\n",
      "Preprocessing file:  AP881030\n",
      "Preprocessing file:  AP881031\n",
      "Preprocessing file:  AP881101\n",
      "Preprocessing file:  AP881102\n",
      "Preprocessing file:  AP881103\n",
      "Preprocessing file:  AP881104\n",
      "Preprocessing file:  AP881105\n",
      "Preprocessing file:  AP881106\n",
      "Preprocessing file:  AP881107\n",
      "Preprocessing file:  AP881108\n",
      "Preprocessing file:  AP881109\n",
      "Preprocessing file:  AP881110\n",
      "Preprocessing file:  AP881111\n",
      "Preprocessing file:  AP881112\n",
      "Preprocessing file:  AP881113\n",
      "Preprocessing file:  AP881114\n",
      "Preprocessing file:  AP881115\n",
      "Preprocessing file:  AP881116\n",
      "Preprocessing file:  AP881117\n",
      "Preprocessing file:  AP881118\n",
      "Preprocessing file:  AP881119\n",
      "Preprocessing file:  AP881120\n",
      "Preprocessing file:  AP881121\n",
      "Preprocessing file:  AP881122\n",
      "Preprocessing file:  AP881123\n",
      "Preprocessing file:  AP881124\n",
      "Preprocessing file:  AP881125\n",
      "Preprocessing file:  AP881126\n",
      "Preprocessing file:  AP881127\n",
      "Preprocessing file:  AP881128\n",
      "Preprocessing file:  AP881129\n",
      "Preprocessing file:  AP881130\n",
      "Preprocessing file:  AP881201\n",
      "Preprocessing file:  AP881202\n",
      "Preprocessing file:  AP881203\n",
      "Preprocessing file:  AP881204\n",
      "Preprocessing file:  AP881205\n",
      "Preprocessing file:  AP881206\n",
      "Preprocessing file:  AP881207\n",
      "Preprocessing file:  AP881208\n",
      "Preprocessing file:  AP881209\n",
      "Preprocessing file:  AP881210\n",
      "Preprocessing file:  AP881211\n",
      "Preprocessing file:  AP881212\n",
      "Preprocessing file:  AP881213\n",
      "Preprocessing file:  AP881214\n",
      "Preprocessing file:  AP881215\n",
      "Preprocessing file:  AP881216\n",
      "Preprocessing file:  AP881217\n",
      "Preprocessing file:  AP881218\n",
      "Preprocessing file:  AP881219\n",
      "Preprocessing file:  AP881220\n",
      "Preprocessing file:  AP881221\n",
      "Preprocessing file:  AP881222\n",
      "Preprocessing file:  AP881223\n",
      "Preprocessing file:  AP881224\n",
      "Preprocessing file:  AP881225\n",
      "Preprocessing file:  AP881226\n",
      "Preprocessing file:  AP881227\n",
      "Preprocessing file:  AP881228\n",
      "Preprocessing file:  AP881229\n",
      "Preprocessing file:  AP881230\n",
      "Preprocessing file:  AP881231\n",
      "preprocessed  322  files\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the collection\n",
    "preprocessed_documents = preprocess_directory('AP_collection/coll')\n",
    "preprocessed_documents.sort(key=lambda x: x.doc_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the topics from the topics file\n",
    "def extract_topics(file, descriptions=False):\n",
    "  with open(file, \"r\") as f:\n",
    "    topic_content = f.read()\n",
    "  all_topics = []\n",
    "  topics = re.findall(r'<top>(.*?)</top>', topic_content, re.DOTALL)\n",
    "  for topic in topics:\n",
    "    raw_title = re.search(r'<title>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "    title = raw_title.group(1) if raw_title else ''\n",
    "    if descriptions:\n",
    "      raw_desc = re.search(r'<desc>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "      desc = raw_desc.group(1) if raw_desc else ''\n",
    "      all_topics.append({'title': title, 'description': desc})\n",
    "    else:\n",
    "      all_topics.append({'title': title})\n",
    "  return all_topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(f'sentence-transformers/{model_name}', device='cpu', cache_folder='./.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def search(n, query, model, preprocessed_documents, doc_embeddings, top_k=20):\n",
    "  # Fetch the embeddings for the query if it exists, otherwise compute it\n",
    "  if os.path.exists(f'embedding_saves/{model_name}/query-{n}.pickle'):\n",
    "    query_embeddings = torch.load(f'embedding_saves/{model_name}/query-{n}.pickle')\n",
    "  else:\n",
    "    query_embeddings = model.encode([query])\n",
    "    os.makedirs(f'embedding_saves/{model_name}', exist_ok=True)\n",
    "    torch.save(query_embeddings, f'embedding_saves/{model_name}/query-{n}.pickle')\n",
    "  # compute distances\n",
    "  distances = scipy.spatial.distance.cdist(query_embeddings, doc_embeddings, \"cosine\")[0]\n",
    "  # get the top k results\n",
    "  results = zip(range(len(distances)), distances)\n",
    "  results = sorted(results, key=lambda x: x[1])\n",
    "  # Create a list of tuples with the document number and the distance\n",
    "  results = [(preprocessed_documents[idx].doc_no, distance) for idx, distance in results[0:top_k]]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished computing embeddings for gtr-t5-xxl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Compute the embeddings\n",
    "for x, doc in enumerate(reversed(preprocessed_documents)):\n",
    "  # Clear the cache\n",
    "  torch.cuda.empty_cache()\n",
    "  # skip the document if it exists, otherwise compute it\n",
    "  if os.path.exists(f'embedding_saves/{model_name}/{doc.doc_no.strip()}.pickle'):\n",
    "    continue\n",
    "  else:\n",
    "    os.makedirs(f'embedding_saves/{model_name}', exist_ok=True)\n",
    "    # Calculate embedding for each document\n",
    "    print(f'Embedding {doc.doc_no.strip()} {x}/{len(preprocessed_documents)}...')\n",
    "    doc_embed = model.encode(doc.doc_text, show_progress_bar=False)\n",
    "    # write the document embedding to a file\n",
    "    with open(f'embedding_saves/{model_name}/{doc.doc_no.strip()}.pickle', 'wb') as f:\n",
    "      pickle.dump(doc_embed, f)\n",
    "print(f'Finished computing embeddings for {model_name}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import gzip\n",
    "# import os\n",
    "\n",
    "# # assuming you have a list of Document objects called documents\n",
    "# # and assuming you have already populated the vector attribute of each Document object\n",
    "\n",
    "# # define the headers for your CSV file\n",
    "# headers = ['doc_no', 'vector']\n",
    "\n",
    "# # open the CSV file in 'w' mode and write the headers\n",
    "# with open(f\"embedding_saves/{model_name}.csv\", mode='w', newline='') as file:\n",
    "#   writer = csv.writer(file)\n",
    "#   writer.writerow(headers)\n",
    "\n",
    "#   # loop through each Document object and write its attributes to the CSV file\n",
    "#   for x, document in enumerate(preprocessed_documents):\n",
    "#     writer.writerow([document.doc_no, doc_embeddings[x]])\n",
    "\n",
    "# # gzip the CSV file\n",
    "# with open(f\"embedding_saves/{model_name}.csv\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.csv.gz\", 'wb') as f_out:\n",
    "#     f_out.writelines(f_in)\n",
    "\n",
    "# os.remove(f\"embedding_saves/{model_name}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/79923\n",
      "1000/79923\n",
      "2000/79923\n",
      "3000/79923\n",
      "4000/79923\n",
      "5000/79923\n",
      "6000/79923\n",
      "7000/79923\n",
      "8000/79923\n",
      "9000/79923\n",
      "10000/79923\n",
      "11000/79923\n",
      "12000/79923\n",
      "13000/79923\n",
      "14000/79923\n",
      "15000/79923\n",
      "16000/79923\n",
      "17000/79923\n",
      "18000/79923\n",
      "19000/79923\n",
      "20000/79923\n",
      "21000/79923\n",
      "22000/79923\n",
      "23000/79923\n",
      "24000/79923\n",
      "25000/79923\n",
      "26000/79923\n",
      "27000/79923\n",
      "28000/79923\n",
      "29000/79923\n",
      "30000/79923\n",
      "31000/79923\n",
      "32000/79923\n",
      "33000/79923\n",
      "34000/79923\n",
      "35000/79923\n",
      "36000/79923\n",
      "37000/79923\n",
      "38000/79923\n",
      "39000/79923\n",
      "40000/79923\n",
      "41000/79923\n",
      "42000/79923\n",
      "43000/79923\n",
      "44000/79923\n",
      "45000/79923\n",
      "46000/79923\n",
      "47000/79923\n",
      "48000/79923\n",
      "49000/79923\n",
      "50000/79923\n",
      "51000/79923\n",
      "52000/79923\n",
      "53000/79923\n",
      "54000/79923\n",
      "55000/79923\n",
      "56000/79923\n",
      "57000/79923\n",
      "58000/79923\n",
      "59000/79923\n",
      "60000/79923\n",
      "61000/79923\n",
      "62000/79923\n",
      "63000/79923\n",
      "64000/79923\n",
      "65000/79923\n",
      "66000/79923\n",
      "67000/79923\n",
      "68000/79923\n",
      "69000/79923\n",
      "70000/79923\n",
      "71000/79923\n",
      "72000/79923\n",
      "73000/79923\n",
      "74000/79923\n",
      "75000/79923\n",
      "76000/79923\n",
      "77000/79923\n",
      "78000/79923\n",
      "79000/79923\n"
     ]
    }
   ],
   "source": [
    "# Read all the embeddings from the files in the directory\n",
    "doc_embeddings = []\n",
    "for x, doc in enumerate(preprocessed_documents):\n",
    "  if x % 1000 == 0:\n",
    "    print(f'{x}/{len(preprocessed_documents)}')\n",
    "  filename = doc.doc_no.strip()\n",
    "  if os.path.exists(f'embedding_saves/{model_name}/{filename}.pickle'):\n",
    "    # print(f'Loading embedding for {model_name}/{filename} {x}/{len(preprocessed_documents)}')\n",
    "    with open(f'embedding_saves/{model_name}/{filename}.pickle', 'rb') as f:\n",
    "      doc_embeddings.append(pickle.load(f))\n",
    "  else:\n",
    "    print(f'Embedding for {model_name}/{filename} doesn\\'t exist {x}/{len(preprocessed_documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Save the embeddings\n",
    "doc_embeddings = np.array(doc_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings computed, saving to file: embedding_saves/gtr-t5-xxl.pickle.gz\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "print(f'Embeddings computed, saving to file: embedding_saves/{model_name}.pickle.gz')\n",
    "# store the embeddings in a pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'wb') as f:\n",
    "  pickle.dump(np.array(doc_embeddings), f)\n",
    "# gzip the pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.pickle.gz\", 'wb') as f_out:\n",
    "  f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all the documents and search for the top 1000 results\n",
    "def query_retrieve(model, preprocessed_documents, doc_embeddings, descriptions=False, runid='runid', filename='Results.txt', top_k=1000):\n",
    "  # Extract the topics\n",
    "  topics = extract_topics('topics1-50.txt', descriptions)\n",
    "\n",
    "  file_out = open(filename, 'w')\n",
    "\n",
    "  for i, topic in enumerate(topics):\n",
    "    print(f'Querying for topic {i+1}...')\n",
    "    # Search for the documents\n",
    "    results = search(i, topic['title'], model, preprocessed_documents, doc_embeddings, top_k)\n",
    "    for j, (doc_id, distance) in enumerate(results):\n",
    "      file_out.write(f'{i+1} Q0 {doc_id.strip()} {j+1} {1-distance} {runid}\\n')\n",
    "  file_out.close()\n",
    "  print('Written results to file ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for topic 1...\n",
      "Querying for topic 2...\n",
      "Querying for topic 3...\n",
      "Querying for topic 4...\n",
      "Querying for topic 5...\n",
      "Querying for topic 6...\n",
      "Querying for topic 7...\n",
      "Querying for topic 8...\n",
      "Querying for topic 9...\n",
      "Querying for topic 10...\n",
      "Querying for topic 11...\n",
      "Querying for topic 12...\n",
      "Querying for topic 13...\n",
      "Querying for topic 14...\n",
      "Querying for topic 15...\n",
      "Querying for topic 16...\n",
      "Querying for topic 17...\n",
      "Querying for topic 18...\n",
      "Querying for topic 19...\n",
      "Querying for topic 20...\n",
      "Querying for topic 21...\n",
      "Querying for topic 22...\n",
      "Querying for topic 23...\n",
      "Querying for topic 24...\n",
      "Querying for topic 25...\n",
      "Querying for topic 26...\n",
      "Querying for topic 27...\n",
      "Querying for topic 28...\n",
      "Querying for topic 29...\n",
      "Querying for topic 30...\n",
      "Querying for topic 31...\n",
      "Querying for topic 32...\n",
      "Querying for topic 33...\n",
      "Querying for topic 34...\n",
      "Querying for topic 35...\n",
      "Querying for topic 36...\n",
      "Querying for topic 37...\n",
      "Querying for topic 38...\n",
      "Querying for topic 39...\n",
      "Querying for topic 40...\n",
      "Querying for topic 41...\n",
      "Querying for topic 42...\n",
      "Querying for topic 43...\n",
      "Querying for topic 44...\n",
      "Querying for topic 45...\n",
      "Querying for topic 46...\n",
      "Querying for topic 47...\n",
      "Querying for topic 48...\n",
      "Querying for topic 49...\n",
      "Querying for topic 50...\n",
      "Written results to file  Results-gtr-t5-xxl.txt\n"
     ]
    }
   ],
   "source": [
    "query_retrieve(model, preprocessed_documents, doc_embeddings, descriptions=False, runid='runid', filename=f'Results-{model_name}.txt', top_k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\trunid\n",
      "num_q                 \tall\t50\n",
      "num_ret               \tall\t50000\n",
      "num_rel               \tall\t2099\n",
      "num_rel_ret           \tall\t1448\n",
      "map                   \tall\t0.3039\n",
      "gm_map                \tall\t0.1876\n",
      "Rprec                 \tall\t0.3178\n",
      "bpref                 \tall\t0.3828\n",
      "recip_rank            \tall\t0.7410\n",
      "iprec_at_recall_0.00  \tall\t0.7757\n",
      "iprec_at_recall_0.10  \tall\t0.6300\n",
      "iprec_at_recall_0.20  \tall\t0.5332\n",
      "iprec_at_recall_0.30  \tall\t0.4262\n",
      "iprec_at_recall_0.40  \tall\t0.3662\n",
      "iprec_at_recall_0.50  \tall\t0.3129\n",
      "iprec_at_recall_0.60  \tall\t0.2194\n",
      "iprec_at_recall_0.70  \tall\t0.1435\n",
      "iprec_at_recall_0.80  \tall\t0.0845\n",
      "iprec_at_recall_0.90  \tall\t0.0438\n",
      "iprec_at_recall_1.00  \tall\t0.0366\n",
      "P_5                   \tall\t0.4920\n",
      "P_10                  \tall\t0.4420\n",
      "P_15                  \tall\t0.4107\n",
      "P_20                  \tall\t0.3860\n",
      "P_30                  \tall\t0.3320\n",
      "P_100                 \tall\t0.1698\n",
      "P_200                 \tall\t0.1019\n",
      "P_500                 \tall\t0.0506\n",
      "P_1000                \tall\t0.0290\n"
     ]
    }
   ],
   "source": [
    "!Powershell.exe -Command \".\\trec_eval qrels1-50ap.txt Results-{model_name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard/.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42b34bead2ece13e82fdd57b33000433053e25f8e38f52c0f662c8d14f00a960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
