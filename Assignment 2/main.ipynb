{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "model_name='all-MiniLM-L12-v2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Howard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions and classes for preprocessing the data\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "  def __init__(self, doc_no, doc_text, tokens):\n",
    "    self.doc_no = doc_no\n",
    "    self.doc_text = doc_text\n",
    "    self.tokens = tokens\n",
    "\n",
    "  def __str__(self):\n",
    "    return 'Document Number: ' + self.doc_no + '\\nDocument Text: ' + self.doc_text + '\\nTokens: ' + str(self.tokens) + '\\n'\n",
    "\n",
    "  def to_dict(self):\n",
    "    return {'docno': self.doc_no, 'doctext': self.doc_text, 'tokens': self.tokens, 'text': ' '.join(self.tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stop words\n",
    "def get_stop_words():\n",
    "  stopwords = set()\n",
    "  # Open the stop words and add them to the set\n",
    "  with open('StopWords.txt', 'r') as file:\n",
    "    for line in file:\n",
    "      stopwords.add(line.strip())\n",
    "  return stopwords\n",
    "\n",
    "\n",
    "# load the stopwords\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "# function to perform preprocessing on the text\n",
    "def preprocess(file):\n",
    "  with open(file, \"r\") as f:\n",
    "    content = f.read()\n",
    "  documents = re.findall(r'<DOC>(.*?)</DOC>', content, re.DOTALL)\n",
    "  preprocessed_documents = []\n",
    "  for document in documents:\n",
    "    # Get the document number and text\n",
    "    raw_no = re.search(r'<DOCNO>(.*?)</DOCNO>', document, re.DOTALL)\n",
    "    doc_no = raw_no.group(1) if raw_no else ''\n",
    "    raw_text = re.search(r'<TEXT>(.*?)</TEXT>', document, re.DOTALL)\n",
    "    doc_text = raw_text.string if raw_text else ''\n",
    "\n",
    "    # create a document object\n",
    "    doc = Document(doc_no, doc_text, [])\n",
    "    preprocessed_documents.append(doc)\n",
    "  return preprocessed_documents\n",
    "\n",
    "# function to preprocess a single text string\n",
    "def preprocess_text(text: str, stem=True, stopwords=True):\n",
    "    # lowercase the text\n",
    "  text = text.lower()\n",
    "\n",
    "  # tokenize the text\n",
    "  tokens = word_tokenize(text)\n",
    "  # remove stopwords\n",
    "  if stopwords:\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "  # stem the tokens\n",
    "  if stem:\n",
    "    # apply the porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "  # remove punctuation\n",
    "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  stripped = [w.translate(table) for w in tokens]\n",
    "  stripped = list(chain(*[w.split() for w in stripped]))\n",
    "\n",
    "  # remove empty tokens, stopwords (if applicable) and non-alphabetic tokens\n",
    "  stripped = [\n",
    "      token for token in stripped if token and (token not in stop_words if stopwords else True) and token.isalpha()]\n",
    "  return stripped\n",
    "\n",
    "# main function to preprocess a directory of text files\n",
    "def preprocess_directory(directory, num_files=-1):\n",
    "  preprocessed_documents = []\n",
    "  ctr = 0\n",
    "  for filename in os.listdir(directory):\n",
    "    print('Preprocessing file: ', filename)\n",
    "    file = os.path.join(directory, filename)\n",
    "    preprocessed_documents.extend(preprocess(file))\n",
    "    ctr += 1\n",
    "    if ctr == num_files and num_files != -1:\n",
    "      break\n",
    "    \n",
    "  print('preprocessed ', ctr, ' files')\n",
    "  return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file:  AP880212\n",
      "Preprocessing file:  AP880213\n",
      "Preprocessing file:  AP880214\n",
      "Preprocessing file:  AP880215\n",
      "Preprocessing file:  AP880216\n",
      "Preprocessing file:  AP880217\n",
      "Preprocessing file:  AP880218\n",
      "Preprocessing file:  AP880219\n",
      "Preprocessing file:  AP880220\n",
      "Preprocessing file:  AP880221\n",
      "Preprocessing file:  AP880222\n",
      "Preprocessing file:  AP880223\n",
      "Preprocessing file:  AP880224\n",
      "Preprocessing file:  AP880225\n",
      "Preprocessing file:  AP880226\n",
      "Preprocessing file:  AP880227\n",
      "Preprocessing file:  AP880228\n",
      "Preprocessing file:  AP880229\n",
      "Preprocessing file:  AP880301\n",
      "Preprocessing file:  AP880302\n",
      "Preprocessing file:  AP880303\n",
      "Preprocessing file:  AP880304\n",
      "Preprocessing file:  AP880307\n",
      "Preprocessing file:  AP880308\n",
      "Preprocessing file:  AP880309\n",
      "Preprocessing file:  AP880310\n",
      "Preprocessing file:  AP880311\n",
      "Preprocessing file:  AP880312\n",
      "Preprocessing file:  AP880313\n",
      "Preprocessing file:  AP880314\n",
      "Preprocessing file:  AP880315\n",
      "Preprocessing file:  AP880316\n",
      "Preprocessing file:  AP880317\n",
      "Preprocessing file:  AP880318\n",
      "Preprocessing file:  AP880319\n",
      "Preprocessing file:  AP880320\n",
      "Preprocessing file:  AP880321\n",
      "Preprocessing file:  AP880322\n",
      "Preprocessing file:  AP880323\n",
      "Preprocessing file:  AP880324\n",
      "Preprocessing file:  AP880325\n",
      "Preprocessing file:  AP880326\n",
      "Preprocessing file:  AP880327\n",
      "Preprocessing file:  AP880328\n",
      "Preprocessing file:  AP880329\n",
      "Preprocessing file:  AP880330\n",
      "Preprocessing file:  AP880331\n",
      "Preprocessing file:  AP880401\n",
      "Preprocessing file:  AP880402\n",
      "Preprocessing file:  AP880403\n",
      "Preprocessing file:  AP880404\n",
      "Preprocessing file:  AP880405\n",
      "Preprocessing file:  AP880406\n",
      "Preprocessing file:  AP880407\n",
      "Preprocessing file:  AP880408\n",
      "Preprocessing file:  AP880409\n",
      "Preprocessing file:  AP880410\n",
      "Preprocessing file:  AP880411\n",
      "Preprocessing file:  AP880412\n",
      "Preprocessing file:  AP880413\n",
      "Preprocessing file:  AP880414\n",
      "Preprocessing file:  AP880415\n",
      "Preprocessing file:  AP880416\n",
      "Preprocessing file:  AP880417\n",
      "Preprocessing file:  AP880418\n",
      "Preprocessing file:  AP880419\n",
      "Preprocessing file:  AP880420\n",
      "Preprocessing file:  AP880421\n",
      "Preprocessing file:  AP880422\n",
      "Preprocessing file:  AP880423\n",
      "Preprocessing file:  AP880424\n",
      "Preprocessing file:  AP880425\n",
      "Preprocessing file:  AP880426\n",
      "Preprocessing file:  AP880427\n",
      "Preprocessing file:  AP880428\n",
      "Preprocessing file:  AP880429\n",
      "Preprocessing file:  AP880430\n",
      "Preprocessing file:  AP880501\n",
      "Preprocessing file:  AP880502\n",
      "Preprocessing file:  AP880503\n",
      "Preprocessing file:  AP880504\n",
      "Preprocessing file:  AP880505\n",
      "Preprocessing file:  AP880506\n",
      "Preprocessing file:  AP880507\n",
      "Preprocessing file:  AP880508\n",
      "Preprocessing file:  AP880509\n",
      "Preprocessing file:  AP880510\n",
      "Preprocessing file:  AP880511\n",
      "Preprocessing file:  AP880512\n",
      "Preprocessing file:  AP880513\n",
      "Preprocessing file:  AP880514\n",
      "Preprocessing file:  AP880515\n",
      "Preprocessing file:  AP880516\n",
      "Preprocessing file:  AP880517\n",
      "Preprocessing file:  AP880518\n",
      "Preprocessing file:  AP880519\n",
      "Preprocessing file:  AP880520\n",
      "Preprocessing file:  AP880521\n",
      "Preprocessing file:  AP880522\n",
      "Preprocessing file:  AP880523\n",
      "Preprocessing file:  AP880524\n",
      "Preprocessing file:  AP880525\n",
      "Preprocessing file:  AP880526\n",
      "Preprocessing file:  AP880527\n",
      "Preprocessing file:  AP880528\n",
      "Preprocessing file:  AP880529\n",
      "Preprocessing file:  AP880530\n",
      "Preprocessing file:  AP880531\n",
      "Preprocessing file:  AP880601\n",
      "Preprocessing file:  AP880602\n",
      "Preprocessing file:  AP880603\n",
      "Preprocessing file:  AP880604\n",
      "Preprocessing file:  AP880605\n",
      "Preprocessing file:  AP880606\n",
      "Preprocessing file:  AP880607\n",
      "Preprocessing file:  AP880608\n",
      "Preprocessing file:  AP880609\n",
      "Preprocessing file:  AP880610\n",
      "Preprocessing file:  AP880611\n",
      "Preprocessing file:  AP880612\n",
      "Preprocessing file:  AP880613\n",
      "Preprocessing file:  AP880614\n",
      "Preprocessing file:  AP880615\n",
      "Preprocessing file:  AP880616\n",
      "Preprocessing file:  AP880617\n",
      "Preprocessing file:  AP880618\n",
      "Preprocessing file:  AP880619\n",
      "Preprocessing file:  AP880620\n",
      "Preprocessing file:  AP880621\n",
      "Preprocessing file:  AP880622\n",
      "Preprocessing file:  AP880623\n",
      "Preprocessing file:  AP880624\n",
      "Preprocessing file:  AP880625\n",
      "Preprocessing file:  AP880626\n",
      "Preprocessing file:  AP880627\n",
      "Preprocessing file:  AP880628\n",
      "Preprocessing file:  AP880629\n",
      "Preprocessing file:  AP880630\n",
      "Preprocessing file:  AP880701\n",
      "Preprocessing file:  AP880702\n",
      "Preprocessing file:  AP880703\n",
      "Preprocessing file:  AP880704\n",
      "Preprocessing file:  AP880705\n",
      "Preprocessing file:  AP880706\n",
      "Preprocessing file:  AP880707\n",
      "Preprocessing file:  AP880708\n",
      "Preprocessing file:  AP880709\n",
      "Preprocessing file:  AP880710\n",
      "Preprocessing file:  AP880711\n",
      "Preprocessing file:  AP880712\n",
      "Preprocessing file:  AP880713\n",
      "Preprocessing file:  AP880714\n",
      "Preprocessing file:  AP880715\n",
      "Preprocessing file:  AP880716\n",
      "Preprocessing file:  AP880717\n",
      "Preprocessing file:  AP880718\n",
      "Preprocessing file:  AP880719\n",
      "Preprocessing file:  AP880720\n",
      "Preprocessing file:  AP880721\n",
      "Preprocessing file:  AP880722\n",
      "Preprocessing file:  AP880723\n",
      "Preprocessing file:  AP880724\n",
      "Preprocessing file:  AP880725\n",
      "Preprocessing file:  AP880726\n",
      "Preprocessing file:  AP880727\n",
      "Preprocessing file:  AP880728\n",
      "Preprocessing file:  AP880729\n",
      "Preprocessing file:  AP880730\n",
      "Preprocessing file:  AP880731\n",
      "Preprocessing file:  AP880801\n",
      "Preprocessing file:  AP880802\n",
      "Preprocessing file:  AP880803\n",
      "Preprocessing file:  AP880804\n",
      "Preprocessing file:  AP880805\n",
      "Preprocessing file:  AP880806\n",
      "Preprocessing file:  AP880807\n",
      "Preprocessing file:  AP880808\n",
      "Preprocessing file:  AP880809\n",
      "Preprocessing file:  AP880810\n",
      "Preprocessing file:  AP880811\n",
      "Preprocessing file:  AP880812\n",
      "Preprocessing file:  AP880813\n",
      "Preprocessing file:  AP880814\n",
      "Preprocessing file:  AP880815\n",
      "Preprocessing file:  AP880816\n",
      "Preprocessing file:  AP880817\n",
      "Preprocessing file:  AP880818\n",
      "Preprocessing file:  AP880819\n",
      "Preprocessing file:  AP880820\n",
      "Preprocessing file:  AP880821\n",
      "Preprocessing file:  AP880822\n",
      "Preprocessing file:  AP880823\n",
      "Preprocessing file:  AP880824\n",
      "Preprocessing file:  AP880825\n",
      "Preprocessing file:  AP880826\n",
      "Preprocessing file:  AP880827\n",
      "Preprocessing file:  AP880828\n",
      "Preprocessing file:  AP880829\n",
      "Preprocessing file:  AP880830\n",
      "Preprocessing file:  AP880831\n",
      "Preprocessing file:  AP880901\n",
      "Preprocessing file:  AP880902\n",
      "Preprocessing file:  AP880903\n",
      "Preprocessing file:  AP880904\n",
      "Preprocessing file:  AP880905\n",
      "Preprocessing file:  AP880906\n",
      "Preprocessing file:  AP880907\n",
      "Preprocessing file:  AP880908\n",
      "Preprocessing file:  AP880909\n",
      "Preprocessing file:  AP880910\n",
      "Preprocessing file:  AP880911\n",
      "Preprocessing file:  AP880912\n",
      "Preprocessing file:  AP880913\n",
      "Preprocessing file:  AP880914\n",
      "Preprocessing file:  AP880915\n",
      "Preprocessing file:  AP880916\n",
      "Preprocessing file:  AP880917\n",
      "Preprocessing file:  AP880918\n",
      "Preprocessing file:  AP880919\n",
      "Preprocessing file:  AP880920\n",
      "Preprocessing file:  AP880921\n",
      "Preprocessing file:  AP880922\n",
      "Preprocessing file:  AP880923\n",
      "Preprocessing file:  AP880924\n",
      "Preprocessing file:  AP880925\n",
      "Preprocessing file:  AP880926\n",
      "Preprocessing file:  AP880927\n",
      "Preprocessing file:  AP880928\n",
      "Preprocessing file:  AP880929\n",
      "Preprocessing file:  AP880930\n",
      "Preprocessing file:  AP881001\n",
      "Preprocessing file:  AP881002\n",
      "Preprocessing file:  AP881003\n",
      "Preprocessing file:  AP881004\n",
      "Preprocessing file:  AP881005\n",
      "Preprocessing file:  AP881006\n",
      "Preprocessing file:  AP881007\n",
      "Preprocessing file:  AP881008\n",
      "Preprocessing file:  AP881009\n",
      "Preprocessing file:  AP881010\n",
      "Preprocessing file:  AP881011\n",
      "Preprocessing file:  AP881012\n",
      "Preprocessing file:  AP881013\n",
      "Preprocessing file:  AP881014\n",
      "Preprocessing file:  AP881015\n",
      "Preprocessing file:  AP881016\n",
      "Preprocessing file:  AP881017\n",
      "Preprocessing file:  AP881018\n",
      "Preprocessing file:  AP881019\n",
      "Preprocessing file:  AP881020\n",
      "Preprocessing file:  AP881021\n",
      "Preprocessing file:  AP881022\n",
      "Preprocessing file:  AP881023\n",
      "Preprocessing file:  AP881024\n",
      "Preprocessing file:  AP881025\n",
      "Preprocessing file:  AP881026\n",
      "Preprocessing file:  AP881027\n",
      "Preprocessing file:  AP881028\n",
      "Preprocessing file:  AP881029\n",
      "Preprocessing file:  AP881030\n",
      "Preprocessing file:  AP881031\n",
      "Preprocessing file:  AP881101\n",
      "Preprocessing file:  AP881102\n",
      "Preprocessing file:  AP881103\n",
      "Preprocessing file:  AP881104\n",
      "Preprocessing file:  AP881105\n",
      "Preprocessing file:  AP881106\n",
      "Preprocessing file:  AP881107\n",
      "Preprocessing file:  AP881108\n",
      "Preprocessing file:  AP881109\n",
      "Preprocessing file:  AP881110\n",
      "Preprocessing file:  AP881111\n",
      "Preprocessing file:  AP881112\n",
      "Preprocessing file:  AP881113\n",
      "Preprocessing file:  AP881114\n",
      "Preprocessing file:  AP881115\n",
      "Preprocessing file:  AP881116\n",
      "Preprocessing file:  AP881117\n",
      "Preprocessing file:  AP881118\n",
      "Preprocessing file:  AP881119\n",
      "Preprocessing file:  AP881120\n",
      "Preprocessing file:  AP881121\n",
      "Preprocessing file:  AP881122\n",
      "Preprocessing file:  AP881123\n",
      "Preprocessing file:  AP881124\n",
      "Preprocessing file:  AP881125\n",
      "Preprocessing file:  AP881126\n",
      "Preprocessing file:  AP881127\n",
      "Preprocessing file:  AP881128\n",
      "Preprocessing file:  AP881129\n",
      "Preprocessing file:  AP881130\n",
      "Preprocessing file:  AP881201\n",
      "Preprocessing file:  AP881202\n",
      "Preprocessing file:  AP881203\n",
      "Preprocessing file:  AP881204\n",
      "Preprocessing file:  AP881205\n",
      "Preprocessing file:  AP881206\n",
      "Preprocessing file:  AP881207\n",
      "Preprocessing file:  AP881208\n",
      "Preprocessing file:  AP881209\n",
      "Preprocessing file:  AP881210\n",
      "Preprocessing file:  AP881211\n",
      "Preprocessing file:  AP881212\n",
      "Preprocessing file:  AP881213\n",
      "Preprocessing file:  AP881214\n",
      "Preprocessing file:  AP881215\n",
      "Preprocessing file:  AP881216\n",
      "Preprocessing file:  AP881217\n",
      "Preprocessing file:  AP881218\n",
      "Preprocessing file:  AP881219\n",
      "Preprocessing file:  AP881220\n",
      "Preprocessing file:  AP881221\n",
      "Preprocessing file:  AP881222\n",
      "Preprocessing file:  AP881223\n",
      "Preprocessing file:  AP881224\n",
      "Preprocessing file:  AP881225\n",
      "Preprocessing file:  AP881226\n",
      "Preprocessing file:  AP881227\n",
      "Preprocessing file:  AP881228\n",
      "Preprocessing file:  AP881229\n",
      "Preprocessing file:  AP881230\n",
      "Preprocessing file:  AP881231\n",
      "preprocessed  322  files\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the collection\n",
    "preprocessed_documents = preprocess_directory('AP_collection/coll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79923"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the topics from the topics file\n",
    "def extract_topics(file, descriptions=False):\n",
    "  with open(file, \"r\") as f:\n",
    "    topic_content = f.read()\n",
    "  all_topics = []\n",
    "  topics = re.findall(r'<top>(.*?)</top>', topic_content, re.DOTALL)\n",
    "  for topic in topics:\n",
    "    raw_title = re.search(r'<title>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "    title = raw_title.group(1) if raw_title else ''\n",
    "    if descriptions:\n",
    "      raw_desc = re.search(r'<desc>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "      desc = raw_desc.group(1) if raw_desc else ''\n",
    "      all_topics.append({'title': title, 'description': desc})\n",
    "    else:\n",
    "      all_topics.append({'title': title})\n",
    "  return all_topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)5dded/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 291kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 47.5kB/s]\n",
      "Downloading (…)4d81d5dded/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.11MB/s]\n",
      "Downloading (…)81d5dded/config.json: 100%|██████████| 573/573 [00:00<00:00, 126kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 30.9kB/s]\n",
      "Downloading (…)ded/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 3.92MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 134M/134M [00:02<00:00, 51.3MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 14.7kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 32.6kB/s]\n",
      "Downloading (…)5dded/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 12.3MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 352/352 [00:00<00:00, 72.3kB/s]\n",
      "Downloading (…)dded/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 2.23MB/s]\n",
      "Downloading (…)4d81d5dded/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 9.39MB/s]\n",
      "Downloading (…)1d5dded/modules.json: 100%|██████████| 349/349 [00:00<00:00, 92.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(f'sentence-transformers/{model_name}', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def search(query, model, preprocessed_documents, doc_embeddings, top_k=20):\n",
    "  query_embeddings = model.encode([query])\n",
    "  # compute distances\n",
    "  distances = scipy.spatial.distance.cdist(query_embeddings, doc_embeddings, \"cosine\")[0]\n",
    "  # get the top k results\n",
    "  results = zip(range(len(distances)), distances)\n",
    "  results = sorted(results, key=lambda x: x[1])\n",
    "  # Create a list of tuples with the document number and the distance\n",
    "  results = [(preprocessed_documents[idx].doc_no, distance) for idx, distance in results[0:top_k]]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding AP880212-0001 0/79923...\n",
      "Embedding AP880212-0002 1/79923...\n",
      "Embedding AP880212-0003 2/79923...\n",
      "Embedding AP880212-0004 3/79923...\n",
      "Embedding AP880212-0005 4/79923...\n",
      "Embedding AP880212-0006 5/79923...\n",
      "Embedding AP880212-0007 6/79923...\n",
      "Embedding AP880212-0008 7/79923...\n",
      "Embedding AP880212-0009 8/79923...\n",
      "Embedding AP880212-0010 9/79923...\n",
      "Embedding AP880212-0011 10/79923...\n",
      "Embedding AP880212-0012 11/79923...\n",
      "Embedding AP880212-0013 12/79923...\n",
      "Embedding AP880212-0014 13/79923...\n",
      "Embedding AP880212-0015 14/79923...\n",
      "Embedding AP880212-0016 15/79923...\n",
      "Embedding AP880212-0017 16/79923...\n",
      "Embedding AP880212-0018 17/79923...\n",
      "Embedding AP880212-0019 18/79923...\n",
      "Embedding AP880212-0020 19/79923...\n",
      "Embedding AP880212-0021 20/79923...\n",
      "Embedding AP880212-0022 21/79923...\n",
      "Embedding AP880212-0023 22/79923...\n",
      "Embedding AP880212-0024 23/79923...\n",
      "Embedding AP880212-0025 24/79923...\n",
      "Embedding AP880212-0026 25/79923...\n",
      "Embedding AP880212-0027 26/79923...\n",
      "Embedding AP880212-0028 27/79923...\n",
      "Embedding AP880212-0029 28/79923...\n",
      "Embedding AP880212-0030 29/79923...\n",
      "Embedding AP880212-0031 30/79923...\n",
      "Embedding AP880212-0032 31/79923...\n",
      "Embedding AP880212-0033 32/79923...\n",
      "Embedding AP880212-0034 33/79923...\n",
      "Embedding AP880212-0035 34/79923...\n",
      "Embedding AP880212-0036 35/79923...\n",
      "Embedding AP880212-0037 36/79923...\n",
      "Embedding AP880212-0038 37/79923...\n",
      "Embedding AP880212-0039 38/79923...\n",
      "Embedding AP880212-0040 39/79923...\n",
      "Embedding AP880212-0041 40/79923...\n",
      "Embedding AP880212-0042 41/79923...\n",
      "Embedding AP880212-0043 42/79923...\n",
      "Embedding AP880212-0044 43/79923...\n",
      "Embedding AP880212-0045 44/79923...\n",
      "Embedding AP880212-0046 45/79923...\n",
      "Embedding AP880212-0047 46/79923...\n",
      "Embedding AP880212-0048 47/79923...\n",
      "Embedding AP880212-0049 48/79923...\n",
      "Embedding AP880212-0050 49/79923...\n",
      "Embedding AP880212-0051 50/79923...\n",
      "Embedding AP880212-0052 51/79923...\n",
      "Embedding AP880212-0053 52/79923...\n",
      "Embedding AP880212-0054 53/79923...\n",
      "Embedding AP880212-0055 54/79923...\n",
      "Embedding AP880212-0056 55/79923...\n",
      "Embedding AP880212-0057 56/79923...\n",
      "Embedding AP880212-0058 57/79923...\n",
      "Embedding AP880212-0059 58/79923...\n",
      "Embedding AP880212-0060 59/79923...\n",
      "Embedding AP880212-0061 60/79923...\n",
      "Embedding AP880212-0062 61/79923...\n",
      "Embedding AP880212-0063 62/79923...\n",
      "Embedding AP880212-0064 63/79923...\n",
      "Embedding AP880212-0065 64/79923...\n",
      "Embedding AP880212-0066 65/79923...\n",
      "Embedding AP880212-0067 66/79923...\n",
      "Embedding AP880212-0068 67/79923...\n",
      "Embedding AP880212-0069 68/79923...\n",
      "Embedding AP880212-0070 69/79923...\n",
      "Embedding AP880212-0071 70/79923...\n",
      "Embedding AP880212-0072 71/79923...\n",
      "Embedding AP880212-0073 72/79923...\n",
      "Embedding AP880212-0074 73/79923...\n",
      "Embedding AP880212-0075 74/79923...\n",
      "Embedding AP880212-0076 75/79923...\n",
      "Embedding AP880212-0077 76/79923...\n",
      "Embedding AP880212-0078 77/79923...\n",
      "Embedding AP880212-0079 78/79923...\n",
      "Embedding AP880212-0080 79/79923...\n",
      "Embedding AP880212-0081 80/79923...\n",
      "Embedding AP880212-0082 81/79923...\n",
      "Embedding AP880212-0083 82/79923...\n",
      "Embedding AP880212-0084 83/79923...\n",
      "Embedding AP880212-0085 84/79923...\n",
      "Embedding AP880212-0086 85/79923...\n",
      "Embedding AP880212-0087 86/79923...\n",
      "Embedding AP880212-0088 87/79923...\n",
      "Embedding AP880212-0089 88/79923...\n",
      "Embedding AP880212-0090 89/79923...\n",
      "Embedding AP880212-0091 90/79923...\n",
      "Embedding AP880212-0092 91/79923...\n",
      "Embedding AP880212-0093 92/79923...\n",
      "Embedding AP880212-0094 93/79923...\n",
      "Embedding AP880212-0095 94/79923...\n",
      "Embedding AP880212-0096 95/79923...\n",
      "Embedding AP880212-0097 96/79923...\n",
      "Embedding AP880212-0098 97/79923...\n",
      "Embedding AP880212-0099 98/79923...\n",
      "Embedding AP880212-0100 99/79923...\n",
      "Embedding AP880212-0101 100/79923...\n",
      "Embedding AP880212-0102 101/79923...\n",
      "Embedding AP880212-0103 102/79923...\n",
      "Embedding AP880212-0104 103/79923...\n",
      "Embedding AP880212-0105 104/79923...\n",
      "Embedding AP880212-0106 105/79923...\n",
      "Embedding AP880212-0107 106/79923...\n",
      "Embedding AP880212-0108 107/79923...\n",
      "Embedding AP880212-0109 108/79923...\n",
      "Embedding AP880212-0110 109/79923...\n",
      "Embedding AP880212-0111 110/79923...\n",
      "Embedding AP880212-0112 111/79923...\n",
      "Embedding AP880212-0113 112/79923...\n",
      "Embedding AP880212-0114 113/79923...\n",
      "Embedding AP880212-0115 114/79923...\n",
      "Embedding AP880212-0116 115/79923...\n",
      "Embedding AP880212-0117 116/79923...\n",
      "Embedding AP880212-0118 117/79923...\n",
      "Embedding AP880212-0119 118/79923...\n",
      "Embedding AP880212-0120 119/79923...\n",
      "Embedding AP880212-0121 120/79923...\n",
      "Embedding AP880212-0122 121/79923...\n",
      "Embedding AP880212-0123 122/79923...\n",
      "Embedding AP880212-0124 123/79923...\n",
      "Embedding AP880212-0125 124/79923...\n",
      "Embedding AP880212-0126 125/79923...\n",
      "Embedding AP880212-0127 126/79923...\n",
      "Embedding AP880212-0128 127/79923...\n",
      "Embedding AP880212-0129 128/79923...\n",
      "Embedding AP880212-0130 129/79923...\n",
      "Embedding AP880212-0131 130/79923...\n",
      "Embedding AP880212-0132 131/79923...\n",
      "Embedding AP880212-0133 132/79923...\n",
      "Embedding AP880212-0134 133/79923...\n",
      "Embedding AP880212-0135 134/79923...\n",
      "Embedding AP880212-0136 135/79923...\n",
      "Embedding AP880212-0137 136/79923...\n",
      "Embedding AP880212-0138 137/79923...\n",
      "Embedding AP880212-0139 138/79923...\n",
      "Embedding AP880212-0140 139/79923...\n",
      "Embedding AP880212-0141 140/79923...\n",
      "Embedding AP880212-0142 141/79923...\n",
      "Embedding AP880212-0143 142/79923...\n",
      "Embedding AP880212-0144 143/79923...\n",
      "Embedding AP880212-0145 144/79923...\n",
      "Embedding AP880212-0146 145/79923...\n",
      "Embedding AP880212-0147 146/79923...\n",
      "Embedding AP880212-0148 147/79923...\n",
      "Embedding AP880212-0149 148/79923...\n",
      "Embedding AP880212-0150 149/79923...\n",
      "Embedding AP880212-0151 150/79923...\n",
      "Embedding AP880212-0152 151/79923...\n",
      "Embedding AP880212-0153 152/79923...\n",
      "Embedding AP880212-0154 153/79923...\n",
      "Embedding AP880212-0155 154/79923...\n",
      "Embedding AP880212-0156 155/79923...\n",
      "Embedding AP880212-0157 156/79923...\n",
      "Embedding AP880212-0158 157/79923...\n",
      "Embedding AP880212-0159 158/79923...\n",
      "Embedding AP880212-0160 159/79923...\n",
      "Embedding AP880212-0161 160/79923...\n",
      "Embedding AP880212-0162 161/79923...\n",
      "Embedding AP880212-0163 162/79923...\n",
      "Embedding AP880212-0164 163/79923...\n",
      "Embedding AP880212-0165 164/79923...\n",
      "Embedding AP880212-0166 165/79923...\n",
      "Embedding AP880212-0167 166/79923...\n",
      "Embedding AP880212-0168 167/79923...\n",
      "Embedding AP880212-0169 168/79923...\n",
      "Embedding AP880212-0170 169/79923...\n",
      "Embedding AP880212-0171 170/79923...\n",
      "Embedding AP880213-0001 171/79923...\n",
      "Embedding AP880213-0002 172/79923...\n",
      "Embedding AP880213-0003 173/79923...\n",
      "Embedding AP880213-0004 174/79923...\n",
      "Embedding AP880213-0005 175/79923...\n",
      "Embedding AP880213-0006 176/79923...\n",
      "Embedding AP880213-0007 177/79923...\n",
      "Embedding AP880213-0008 178/79923...\n",
      "Embedding AP880213-0009 179/79923...\n",
      "Embedding AP880213-0010 180/79923...\n",
      "Embedding AP880213-0011 181/79923...\n",
      "Embedding AP880213-0012 182/79923...\n",
      "Embedding AP880213-0013 183/79923...\n",
      "Embedding AP880213-0014 184/79923...\n",
      "Embedding AP880213-0015 185/79923...\n",
      "Embedding AP880213-0016 186/79923...\n",
      "Embedding AP880213-0017 187/79923...\n",
      "Embedding AP880213-0018 188/79923...\n",
      "Embedding AP880213-0019 189/79923...\n",
      "Embedding AP880213-0020 190/79923...\n",
      "Embedding AP880213-0021 191/79923...\n",
      "Embedding AP880213-0022 192/79923...\n",
      "Embedding AP880213-0023 193/79923...\n",
      "Embedding AP880213-0024 194/79923...\n",
      "Embedding AP880213-0025 195/79923...\n",
      "Embedding AP880213-0026 196/79923...\n",
      "Embedding AP880213-0027 197/79923...\n",
      "Embedding AP880213-0028 198/79923...\n",
      "Embedding AP880213-0029 199/79923...\n",
      "Embedding AP880213-0030 200/79923...\n",
      "Embedding AP880213-0031 201/79923...\n",
      "Embedding AP880213-0032 202/79923...\n",
      "Embedding AP880213-0033 203/79923...\n",
      "Embedding AP880213-0034 204/79923...\n",
      "Embedding AP880213-0035 205/79923...\n",
      "Embedding AP880213-0036 206/79923...\n",
      "Embedding AP880213-0037 207/79923...\n",
      "Embedding AP880213-0038 208/79923...\n",
      "Embedding AP880213-0039 209/79923...\n",
      "Embedding AP880213-0040 210/79923...\n",
      "Embedding AP880213-0041 211/79923...\n",
      "Embedding AP880213-0042 212/79923...\n",
      "Embedding AP880213-0043 213/79923...\n",
      "Embedding AP880213-0044 214/79923...\n",
      "Embedding AP880213-0045 215/79923...\n",
      "Embedding AP880213-0046 216/79923...\n",
      "Embedding AP880213-0047 217/79923...\n",
      "Embedding AP880213-0048 218/79923...\n",
      "Embedding AP880213-0049 219/79923...\n",
      "Embedding AP880213-0050 220/79923...\n",
      "Embedding AP880213-0051 221/79923...\n",
      "Embedding AP880213-0052 222/79923...\n",
      "Embedding AP880213-0053 223/79923...\n",
      "Embedding AP880213-0054 224/79923...\n",
      "Embedding AP880213-0055 225/79923...\n",
      "Embedding AP880213-0056 226/79923...\n",
      "Embedding AP880213-0057 227/79923...\n",
      "Embedding AP880213-0058 228/79923...\n",
      "Embedding AP880213-0059 229/79923...\n",
      "Embedding AP880213-0060 230/79923...\n",
      "Embedding AP880213-0061 231/79923...\n",
      "Embedding AP880213-0062 232/79923...\n",
      "Embedding AP880213-0063 233/79923...\n",
      "Embedding AP880213-0064 234/79923...\n",
      "Embedding AP880213-0065 235/79923...\n",
      "Embedding AP880213-0066 236/79923...\n",
      "Embedding AP880213-0067 237/79923...\n",
      "Embedding AP880213-0068 238/79923...\n",
      "Embedding AP880213-0069 239/79923...\n",
      "Embedding AP880213-0070 240/79923...\n",
      "Embedding AP880213-0071 241/79923...\n",
      "Embedding AP880213-0072 242/79923...\n",
      "Embedding AP880213-0073 243/79923...\n",
      "Embedding AP880213-0074 244/79923...\n",
      "Embedding AP880213-0075 245/79923...\n",
      "Embedding AP880213-0076 246/79923...\n",
      "Embedding AP880213-0077 247/79923...\n",
      "Embedding AP880213-0078 248/79923...\n",
      "Embedding AP880213-0079 249/79923...\n",
      "Embedding AP880213-0080 250/79923...\n",
      "Embedding AP880213-0081 251/79923...\n",
      "Embedding AP880213-0082 252/79923...\n",
      "Embedding AP880213-0083 253/79923...\n",
      "Embedding AP880213-0084 254/79923...\n",
      "Embedding AP880213-0085 255/79923...\n",
      "Embedding AP880213-0086 256/79923...\n",
      "Embedding AP880213-0087 257/79923...\n",
      "Embedding AP880213-0088 258/79923...\n",
      "Embedding AP880213-0089 259/79923...\n",
      "Embedding AP880213-0090 260/79923...\n",
      "Embedding AP880213-0091 261/79923...\n",
      "Embedding AP880213-0092 262/79923...\n",
      "Embedding AP880213-0093 263/79923...\n",
      "Embedding AP880213-0094 264/79923...\n",
      "Embedding AP880213-0095 265/79923...\n",
      "Embedding AP880213-0096 266/79923...\n",
      "Embedding AP880213-0097 267/79923...\n",
      "Embedding AP880213-0098 268/79923...\n",
      "Embedding AP880213-0099 269/79923...\n",
      "Embedding AP880213-0100 270/79923...\n",
      "Embedding AP880213-0101 271/79923...\n",
      "Embedding AP880213-0102 272/79923...\n",
      "Embedding AP880213-0103 273/79923...\n",
      "Embedding AP880213-0104 274/79923...\n",
      "Embedding AP880213-0105 275/79923...\n",
      "Embedding AP880213-0106 276/79923...\n",
      "Embedding AP880213-0107 277/79923...\n",
      "Embedding AP880213-0108 278/79923...\n",
      "Embedding AP880213-0109 279/79923...\n",
      "Embedding AP880213-0110 280/79923...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# Calculate embedding for each document\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEmbedding \u001b[39m\u001b[39m{\u001b[39;00mdoc\u001b[39m.\u001b[39mdoc_no\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(preprocessed_documents)\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m doc_embed \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(doc\u001b[39m.\u001b[39;49mdoc_text, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# write the document embedding to a file\u001b[39;00m\n\u001b[0;32m     14\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39membeddings/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:161\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m start_index \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(sentences), batch_size, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatches\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    160\u001b[0m     sentences_batch \u001b[39m=\u001b[39m sentences_sorted[start_index:start_index\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m--> 161\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenize(sentences_batch)\n\u001b[0;32m    162\u001b[0m     features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:319\u001b[0m, in \u001b[0;36mSentenceTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, texts: Union[List[\u001b[39mstr\u001b[39m], List[Dict], List[Tuple[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]]):\n\u001b[0;32m    316\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39m    Tokenizes the texts\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_first_module()\u001b[39m.\u001b[39;49mtokenize(texts)\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:113\u001b[0m, in \u001b[0;36mTransformer.tokenize\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case:\n\u001b[0;32m    111\u001b[0m     to_tokenize \u001b[39m=\u001b[39m [[s\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m to_tokenize]\n\u001b[1;32m--> 113\u001b[0m output\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\u001b[39m*\u001b[39;49mto_tokenize, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest_first\u001b[39;49m\u001b[39m'\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_seq_length))\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2529\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2530\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[0;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2616\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2611\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2612\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2613\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2614\u001b[0m         )\n\u001b[0;32m   2615\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[0;32m   2617\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2618\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2619\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2620\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2621\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2622\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2623\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2624\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2625\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2626\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2627\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2628\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2629\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2630\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2631\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2632\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2633\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2634\u001b[0m     )\n\u001b[0;32m   2635\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2636\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2637\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2638\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2655\u001b[0m     )\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2807\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2797\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2798\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2799\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2800\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2804\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2805\u001b[0m )\n\u001b[1;32m-> 2807\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[0;32m   2808\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2809\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2810\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m   2811\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m   2812\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2813\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2814\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2815\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2816\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2817\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2818\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2819\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2820\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2821\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2822\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2823\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2824\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2825\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:428\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    421\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    422\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    426\u001b[0m )\n\u001b[1;32m--> 428\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[0;32m    429\u001b[0m     batch_text_or_text_pairs,\n\u001b[0;32m    430\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    431\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    432\u001b[0m )\n\u001b[0;32m    434\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    440\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[0;32m    441\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[0;32m    442\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[0;32m    452\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "# Compute the embeddings\n",
    "doc_embeddings = []\n",
    "for x, doc in enumerate(preprocessed_documents):\n",
    "  # Clear the cache\n",
    "  torch.cuda.empty_cache()\n",
    "  # Calculate embedding for each document\n",
    "  print(f'Embedding {doc.doc_no.strip()} {x}/{len(preprocessed_documents)}...')\n",
    "  doc_embed = model.encode(doc.doc_text, show_progress_bar=False)\n",
    "  # write the document embedding to a file\n",
    "  os.makedirs(f'embedding_saves/{model_name}', exist_ok=True)\n",
    "  with open(f'embedding_saves/{model_name}/{doc.doc_no.strip()}.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_embed, f)\n",
    "  doc_embeddings.append(doc_embed)\n",
    "\n",
    "# Save the embeddings\n",
    "doc_embeddings = np.array(doc_embeddings)\n",
    "\n",
    "# store the embeddings in a pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'wb') as f:\n",
    "  pickle.dump(doc_embeddings, f)\n",
    "# gzip the pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.pickle.gz\", 'wb') as f_out:\n",
    "  f_out.writelines(f_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import gzip\n",
    "# import os\n",
    "\n",
    "# # assuming you have a list of Document objects called documents\n",
    "# # and assuming you have already populated the vector attribute of each Document object\n",
    "\n",
    "# # define the headers for your CSV file\n",
    "# headers = ['doc_no', 'vector']\n",
    "\n",
    "# # open the CSV file in 'w' mode and write the headers\n",
    "# with open(f\"embedding_saves/{model_name}.csv\", mode='w', newline='') as file:\n",
    "#   writer = csv.writer(file)\n",
    "#   writer.writerow(headers)\n",
    "\n",
    "#   # loop through each Document object and write its attributes to the CSV file\n",
    "#   for x, document in enumerate(preprocessed_documents):\n",
    "#     writer.writerow([document.doc_no, doc_embeddings[x]])\n",
    "\n",
    "# # gzip the CSV file\n",
    "# with open(f\"embedding_saves/{model_name}.csv\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.csv.gz\", 'wb') as f_out:\n",
    "#     f_out.writelines(f_in)\n",
    "\n",
    "# os.remove(f\"embedding_saves/{model_name}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all the documents and search for the top 1000 results\n",
    "def query_retrieve(model, preprocessed_documents, doc_embeddings, descriptions=False, runid='runid', filename='Results.txt', top_k=1000):\n",
    "  # Extract the topics\n",
    "  topics = extract_topics('topics1-50.txt', descriptions)\n",
    "\n",
    "  file_out = open(filename, 'w')\n",
    "\n",
    "  for i, topic in enumerate(topics):\n",
    "    # Search for the documents\n",
    "    results = search(topic['title'], model, preprocessed_documents, doc_embeddings, top_k)\n",
    "    for j, (doc_id, distance) in enumerate(results):\n",
    "      file_out.write(f'{i+1} Q0 {doc_id.strip()} {j+1} {1-distance} {runid}\\n')\n",
    "  file_out.close()\n",
    "  print('Written results to file ', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written results to file  Results-all-MiniLM-L12-v2.txt\n"
     ]
    }
   ],
   "source": [
    "query_retrieve(model, preprocessed_documents, doc_embeddings, descriptions=False, runid='runid', filename=f'Results-{model_name}.txt', top_k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\trunid\n",
      "num_q                 \tall\t50\n",
      "num_ret               \tall\t50000\n",
      "num_rel               \tall\t2099\n",
      "num_rel_ret           \tall\t1144\n",
      "map                   \tall\t0.1705\n",
      "gm_map                \tall\t0.0644\n",
      "Rprec                 \tall\t0.2044\n",
      "bpref                 \tall\t0.2731\n",
      "recip_rank            \tall\t0.5510\n",
      "iprec_at_recall_0.00  \tall\t0.5854\n",
      "iprec_at_recall_0.10  \tall\t0.3964\n",
      "iprec_at_recall_0.20  \tall\t0.3209\n",
      "iprec_at_recall_0.30  \tall\t0.2502\n",
      "iprec_at_recall_0.40  \tall\t0.1879\n",
      "iprec_at_recall_0.50  \tall\t0.1266\n",
      "iprec_at_recall_0.60  \tall\t0.0794\n",
      "iprec_at_recall_0.70  \tall\t0.0611\n",
      "iprec_at_recall_0.80  \tall\t0.0429\n",
      "iprec_at_recall_0.90  \tall\t0.0341\n",
      "iprec_at_recall_1.00  \tall\t0.0202\n",
      "P_5                   \tall\t0.3360\n",
      "P_10                  \tall\t0.3040\n",
      "P_15                  \tall\t0.2840\n",
      "P_20                  \tall\t0.2520\n",
      "P_30                  \tall\t0.2293\n",
      "P_100                 \tall\t0.1248\n",
      "P_200                 \tall\t0.0778\n",
      "P_500                 \tall\t0.0391\n",
      "P_1000                \tall\t0.0229\n"
     ]
    }
   ],
   "source": [
    "!Powershell.exe -Command \".\\trec_eval qrels1-50ap.txt Results-{model_name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Howard/.cache\\huggingface\\hub\n"
     ]
    }
   ],
   "source": [
    "from transformers import file_utils\n",
    "print(file_utils.default_cache_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42b34bead2ece13e82fdd57b33000433053e25f8e38f52c0f662c8d14f00a960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
