{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Howard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions and classes for preprocessing the data\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "  def __init__(self, doc_no, doc_text, tokens):\n",
    "    self.doc_no = doc_no\n",
    "    self.doc_text = doc_text\n",
    "    self.tokens = tokens\n",
    "\n",
    "  def __str__(self):\n",
    "    return 'Document Number: ' + self.doc_no + '\\nDocument Text: ' + self.doc_text + '\\nTokens: ' + str(self.tokens) + '\\n'\n",
    "\n",
    "  def to_dict(self):\n",
    "    return {'docno': self.doc_no, 'doctext': self.doc_text, 'tokens': self.tokens, 'text': ' '.join(self.tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stop words\n",
    "def get_stop_words():\n",
    "  stopwords = set()\n",
    "  # Open the stop words and add them to the set\n",
    "  with open('StopWords.txt', 'r') as file:\n",
    "    for line in file:\n",
    "      stopwords.add(line.strip())\n",
    "  return stopwords\n",
    "\n",
    "\n",
    "# load the stopwords\n",
    "stop_words = get_stop_words()\n",
    "\n",
    "# function to perform preprocessing on the text\n",
    "def preprocess(file):\n",
    "  with open(file, \"r\") as f:\n",
    "    content = f.read()\n",
    "  documents = re.findall(r'<DOC>(.*?)</DOC>', content, re.DOTALL)\n",
    "  preprocessed_documents = []\n",
    "  for document in documents:\n",
    "    # Get the document number and text\n",
    "    raw_no = re.search(r'<DOCNO>(.*?)</DOCNO>', document, re.DOTALL)\n",
    "    doc_no = raw_no.group(1) if raw_no else ''\n",
    "    raw_text = re.search(r'<TEXT>(.*?)</TEXT>', document, re.DOTALL)\n",
    "    doc_text = raw_text.string if raw_text else ''\n",
    "\n",
    "    # create a document object\n",
    "    doc = Document(doc_no, doc_text, [])\n",
    "    preprocessed_documents.append(doc)\n",
    "  return preprocessed_documents\n",
    "\n",
    "# function to preprocess a single text string\n",
    "def preprocess_text(text: str, stem=True, stopwords=True):\n",
    "    # lowercase the text\n",
    "  text = text.lower()\n",
    "\n",
    "  # tokenize the text\n",
    "  tokens = word_tokenize(text)\n",
    "  # remove stopwords\n",
    "  if stopwords:\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "  # stem the tokens\n",
    "  if stem:\n",
    "    # apply the porter stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "  # remove punctuation\n",
    "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  stripped = [w.translate(table) for w in tokens]\n",
    "  stripped = list(chain(*[w.split() for w in stripped]))\n",
    "\n",
    "  # remove empty tokens, stopwords (if applicable) and non-alphabetic tokens\n",
    "  stripped = [\n",
    "      token for token in stripped if token and (token not in stop_words if stopwords else True) and token.isalpha()]\n",
    "  return stripped\n",
    "\n",
    "# main function to preprocess a directory of text files\n",
    "def preprocess_directory(directory, num_files=-1):\n",
    "  preprocessed_documents = []\n",
    "  ctr = 0\n",
    "  for filename in os.listdir(directory):\n",
    "    print('Preprocessing file: ', filename)\n",
    "    file = os.path.join(directory, filename)\n",
    "    preprocessed_documents.extend(preprocess(file))\n",
    "    ctr += 1\n",
    "    if ctr == num_files and num_files != -1:\n",
    "      break\n",
    "    \n",
    "  print('preprocessed ', ctr, ' files')\n",
    "  return preprocessed_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing file:  AP880212\n",
      "Preprocessing file:  AP880213\n",
      "Preprocessing file:  AP880214\n",
      "Preprocessing file:  AP880215\n",
      "Preprocessing file:  AP880216\n",
      "Preprocessing file:  AP880217\n",
      "Preprocessing file:  AP880218\n",
      "Preprocessing file:  AP880219\n",
      "Preprocessing file:  AP880220\n",
      "Preprocessing file:  AP880221\n",
      "Preprocessing file:  AP880222\n",
      "Preprocessing file:  AP880223\n",
      "Preprocessing file:  AP880224\n",
      "Preprocessing file:  AP880225\n",
      "Preprocessing file:  AP880226\n",
      "Preprocessing file:  AP880227\n",
      "Preprocessing file:  AP880228\n",
      "Preprocessing file:  AP880229\n",
      "Preprocessing file:  AP880301\n",
      "Preprocessing file:  AP880302\n",
      "Preprocessing file:  AP880303\n",
      "Preprocessing file:  AP880304\n",
      "Preprocessing file:  AP880307\n",
      "Preprocessing file:  AP880308\n",
      "Preprocessing file:  AP880309\n",
      "Preprocessing file:  AP880310\n",
      "Preprocessing file:  AP880311\n",
      "Preprocessing file:  AP880312\n",
      "Preprocessing file:  AP880313\n",
      "Preprocessing file:  AP880314\n",
      "Preprocessing file:  AP880315\n",
      "Preprocessing file:  AP880316\n",
      "Preprocessing file:  AP880317\n",
      "Preprocessing file:  AP880318\n",
      "Preprocessing file:  AP880319\n",
      "Preprocessing file:  AP880320\n",
      "Preprocessing file:  AP880321\n",
      "Preprocessing file:  AP880322\n",
      "Preprocessing file:  AP880323\n",
      "Preprocessing file:  AP880324\n",
      "Preprocessing file:  AP880325\n",
      "Preprocessing file:  AP880326\n",
      "Preprocessing file:  AP880327\n",
      "Preprocessing file:  AP880328\n",
      "Preprocessing file:  AP880329\n",
      "Preprocessing file:  AP880330\n",
      "Preprocessing file:  AP880331\n",
      "Preprocessing file:  AP880401\n",
      "Preprocessing file:  AP880402\n",
      "Preprocessing file:  AP880403\n",
      "Preprocessing file:  AP880404\n",
      "Preprocessing file:  AP880405\n",
      "Preprocessing file:  AP880406\n",
      "Preprocessing file:  AP880407\n",
      "Preprocessing file:  AP880408\n",
      "Preprocessing file:  AP880409\n",
      "Preprocessing file:  AP880410\n",
      "Preprocessing file:  AP880411\n",
      "Preprocessing file:  AP880412\n",
      "Preprocessing file:  AP880413\n",
      "Preprocessing file:  AP880414\n",
      "Preprocessing file:  AP880415\n",
      "Preprocessing file:  AP880416\n",
      "Preprocessing file:  AP880417\n",
      "Preprocessing file:  AP880418\n",
      "Preprocessing file:  AP880419\n",
      "Preprocessing file:  AP880420\n",
      "Preprocessing file:  AP880421\n",
      "Preprocessing file:  AP880422\n",
      "Preprocessing file:  AP880423\n",
      "Preprocessing file:  AP880424\n",
      "Preprocessing file:  AP880425\n",
      "Preprocessing file:  AP880426\n",
      "Preprocessing file:  AP880427\n",
      "Preprocessing file:  AP880428\n",
      "Preprocessing file:  AP880429\n",
      "Preprocessing file:  AP880430\n",
      "Preprocessing file:  AP880501\n",
      "Preprocessing file:  AP880502\n",
      "Preprocessing file:  AP880503\n",
      "Preprocessing file:  AP880504\n",
      "Preprocessing file:  AP880505\n",
      "Preprocessing file:  AP880506\n",
      "Preprocessing file:  AP880507\n",
      "Preprocessing file:  AP880508\n",
      "Preprocessing file:  AP880509\n",
      "Preprocessing file:  AP880510\n",
      "Preprocessing file:  AP880511\n",
      "Preprocessing file:  AP880512\n",
      "Preprocessing file:  AP880513\n",
      "Preprocessing file:  AP880514\n",
      "Preprocessing file:  AP880515\n",
      "Preprocessing file:  AP880516\n",
      "Preprocessing file:  AP880517\n",
      "Preprocessing file:  AP880518\n",
      "Preprocessing file:  AP880519\n",
      "Preprocessing file:  AP880520\n",
      "Preprocessing file:  AP880521\n",
      "Preprocessing file:  AP880522\n",
      "Preprocessing file:  AP880523\n",
      "Preprocessing file:  AP880524\n",
      "Preprocessing file:  AP880525\n",
      "Preprocessing file:  AP880526\n",
      "Preprocessing file:  AP880527\n",
      "Preprocessing file:  AP880528\n",
      "Preprocessing file:  AP880529\n",
      "Preprocessing file:  AP880530\n",
      "Preprocessing file:  AP880531\n",
      "Preprocessing file:  AP880601\n",
      "Preprocessing file:  AP880602\n",
      "Preprocessing file:  AP880603\n",
      "Preprocessing file:  AP880604\n",
      "Preprocessing file:  AP880605\n",
      "Preprocessing file:  AP880606\n",
      "Preprocessing file:  AP880607\n",
      "Preprocessing file:  AP880608\n",
      "Preprocessing file:  AP880609\n",
      "Preprocessing file:  AP880610\n",
      "Preprocessing file:  AP880611\n",
      "Preprocessing file:  AP880612\n",
      "Preprocessing file:  AP880613\n",
      "Preprocessing file:  AP880614\n",
      "Preprocessing file:  AP880615\n",
      "Preprocessing file:  AP880616\n",
      "Preprocessing file:  AP880617\n",
      "Preprocessing file:  AP880618\n",
      "Preprocessing file:  AP880619\n",
      "Preprocessing file:  AP880620\n",
      "Preprocessing file:  AP880621\n",
      "Preprocessing file:  AP880622\n",
      "Preprocessing file:  AP880623\n",
      "Preprocessing file:  AP880624\n",
      "Preprocessing file:  AP880625\n",
      "Preprocessing file:  AP880626\n",
      "Preprocessing file:  AP880627\n",
      "Preprocessing file:  AP880628\n",
      "Preprocessing file:  AP880629\n",
      "Preprocessing file:  AP880630\n",
      "Preprocessing file:  AP880701\n",
      "Preprocessing file:  AP880702\n",
      "Preprocessing file:  AP880703\n",
      "Preprocessing file:  AP880704\n",
      "Preprocessing file:  AP880705\n",
      "Preprocessing file:  AP880706\n",
      "Preprocessing file:  AP880707\n",
      "Preprocessing file:  AP880708\n",
      "Preprocessing file:  AP880709\n",
      "Preprocessing file:  AP880710\n",
      "Preprocessing file:  AP880711\n",
      "Preprocessing file:  AP880712\n",
      "Preprocessing file:  AP880713\n",
      "Preprocessing file:  AP880714\n",
      "Preprocessing file:  AP880715\n",
      "Preprocessing file:  AP880716\n",
      "Preprocessing file:  AP880717\n",
      "Preprocessing file:  AP880718\n",
      "Preprocessing file:  AP880719\n",
      "Preprocessing file:  AP880720\n",
      "Preprocessing file:  AP880721\n",
      "Preprocessing file:  AP880722\n",
      "Preprocessing file:  AP880723\n",
      "Preprocessing file:  AP880724\n",
      "Preprocessing file:  AP880725\n",
      "Preprocessing file:  AP880726\n",
      "Preprocessing file:  AP880727\n",
      "Preprocessing file:  AP880728\n",
      "Preprocessing file:  AP880729\n",
      "Preprocessing file:  AP880730\n",
      "Preprocessing file:  AP880731\n",
      "Preprocessing file:  AP880801\n",
      "Preprocessing file:  AP880802\n",
      "Preprocessing file:  AP880803\n",
      "Preprocessing file:  AP880804\n",
      "Preprocessing file:  AP880805\n",
      "Preprocessing file:  AP880806\n",
      "Preprocessing file:  AP880807\n",
      "Preprocessing file:  AP880808\n",
      "Preprocessing file:  AP880809\n",
      "Preprocessing file:  AP880810\n",
      "Preprocessing file:  AP880811\n",
      "Preprocessing file:  AP880812\n",
      "Preprocessing file:  AP880813\n",
      "Preprocessing file:  AP880814\n",
      "Preprocessing file:  AP880815\n",
      "Preprocessing file:  AP880816\n",
      "Preprocessing file:  AP880817\n",
      "Preprocessing file:  AP880818\n",
      "Preprocessing file:  AP880819\n",
      "Preprocessing file:  AP880820\n",
      "Preprocessing file:  AP880821\n",
      "Preprocessing file:  AP880822\n",
      "Preprocessing file:  AP880823\n",
      "Preprocessing file:  AP880824\n",
      "Preprocessing file:  AP880825\n",
      "Preprocessing file:  AP880826\n",
      "Preprocessing file:  AP880827\n",
      "Preprocessing file:  AP880828\n",
      "Preprocessing file:  AP880829\n",
      "Preprocessing file:  AP880830\n",
      "Preprocessing file:  AP880831\n",
      "Preprocessing file:  AP880901\n",
      "Preprocessing file:  AP880902\n",
      "Preprocessing file:  AP880903\n",
      "Preprocessing file:  AP880904\n",
      "Preprocessing file:  AP880905\n",
      "Preprocessing file:  AP880906\n",
      "Preprocessing file:  AP880907\n",
      "Preprocessing file:  AP880908\n",
      "Preprocessing file:  AP880909\n",
      "Preprocessing file:  AP880910\n",
      "Preprocessing file:  AP880911\n",
      "Preprocessing file:  AP880912\n",
      "Preprocessing file:  AP880913\n",
      "Preprocessing file:  AP880914\n",
      "Preprocessing file:  AP880915\n",
      "Preprocessing file:  AP880916\n",
      "Preprocessing file:  AP880917\n",
      "Preprocessing file:  AP880918\n",
      "Preprocessing file:  AP880919\n",
      "Preprocessing file:  AP880920\n",
      "Preprocessing file:  AP880921\n",
      "Preprocessing file:  AP880922\n",
      "Preprocessing file:  AP880923\n",
      "Preprocessing file:  AP880924\n",
      "Preprocessing file:  AP880925\n",
      "Preprocessing file:  AP880926\n",
      "Preprocessing file:  AP880927\n",
      "Preprocessing file:  AP880928\n",
      "Preprocessing file:  AP880929\n",
      "Preprocessing file:  AP880930\n",
      "Preprocessing file:  AP881001\n",
      "Preprocessing file:  AP881002\n",
      "Preprocessing file:  AP881003\n",
      "Preprocessing file:  AP881004\n",
      "Preprocessing file:  AP881005\n",
      "Preprocessing file:  AP881006\n",
      "Preprocessing file:  AP881007\n",
      "Preprocessing file:  AP881008\n",
      "Preprocessing file:  AP881009\n",
      "Preprocessing file:  AP881010\n",
      "Preprocessing file:  AP881011\n",
      "Preprocessing file:  AP881012\n",
      "Preprocessing file:  AP881013\n",
      "Preprocessing file:  AP881014\n",
      "Preprocessing file:  AP881015\n",
      "Preprocessing file:  AP881016\n",
      "Preprocessing file:  AP881017\n",
      "Preprocessing file:  AP881018\n",
      "Preprocessing file:  AP881019\n",
      "Preprocessing file:  AP881020\n",
      "Preprocessing file:  AP881021\n",
      "Preprocessing file:  AP881022\n",
      "Preprocessing file:  AP881023\n",
      "Preprocessing file:  AP881024\n",
      "Preprocessing file:  AP881025\n",
      "Preprocessing file:  AP881026\n",
      "Preprocessing file:  AP881027\n",
      "Preprocessing file:  AP881028\n",
      "Preprocessing file:  AP881029\n",
      "Preprocessing file:  AP881030\n",
      "Preprocessing file:  AP881031\n",
      "Preprocessing file:  AP881101\n",
      "Preprocessing file:  AP881102\n",
      "Preprocessing file:  AP881103\n",
      "Preprocessing file:  AP881104\n",
      "Preprocessing file:  AP881105\n",
      "Preprocessing file:  AP881106\n",
      "Preprocessing file:  AP881107\n",
      "Preprocessing file:  AP881108\n",
      "Preprocessing file:  AP881109\n",
      "Preprocessing file:  AP881110\n",
      "Preprocessing file:  AP881111\n",
      "Preprocessing file:  AP881112\n",
      "Preprocessing file:  AP881113\n",
      "Preprocessing file:  AP881114\n",
      "Preprocessing file:  AP881115\n",
      "Preprocessing file:  AP881116\n",
      "Preprocessing file:  AP881117\n",
      "Preprocessing file:  AP881118\n",
      "Preprocessing file:  AP881119\n",
      "Preprocessing file:  AP881120\n",
      "Preprocessing file:  AP881121\n",
      "Preprocessing file:  AP881122\n",
      "Preprocessing file:  AP881123\n",
      "Preprocessing file:  AP881124\n",
      "Preprocessing file:  AP881125\n",
      "Preprocessing file:  AP881126\n",
      "Preprocessing file:  AP881127\n",
      "Preprocessing file:  AP881128\n",
      "Preprocessing file:  AP881129\n",
      "Preprocessing file:  AP881130\n",
      "Preprocessing file:  AP881201\n",
      "Preprocessing file:  AP881202\n",
      "Preprocessing file:  AP881203\n",
      "Preprocessing file:  AP881204\n",
      "Preprocessing file:  AP881205\n",
      "Preprocessing file:  AP881206\n",
      "Preprocessing file:  AP881207\n",
      "Preprocessing file:  AP881208\n",
      "Preprocessing file:  AP881209\n",
      "Preprocessing file:  AP881210\n",
      "Preprocessing file:  AP881211\n",
      "Preprocessing file:  AP881212\n",
      "Preprocessing file:  AP881213\n",
      "Preprocessing file:  AP881214\n",
      "Preprocessing file:  AP881215\n",
      "Preprocessing file:  AP881216\n",
      "Preprocessing file:  AP881217\n",
      "Preprocessing file:  AP881218\n",
      "Preprocessing file:  AP881219\n",
      "Preprocessing file:  AP881220\n",
      "Preprocessing file:  AP881221\n",
      "Preprocessing file:  AP881222\n",
      "Preprocessing file:  AP881223\n",
      "Preprocessing file:  AP881224\n",
      "Preprocessing file:  AP881225\n",
      "Preprocessing file:  AP881226\n",
      "Preprocessing file:  AP881227\n",
      "Preprocessing file:  AP881228\n",
      "Preprocessing file:  AP881229\n",
      "Preprocessing file:  AP881230\n",
      "Preprocessing file:  AP881231\n",
      "preprocessed  322  files\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the collection\n",
    "preprocessed_documents = preprocess_directory('AP_collection/coll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79923"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the topics from the topics file\n",
    "def extract_topics(file, descriptions=False):\n",
    "  with open(file, \"r\") as f:\n",
    "    topic_content = f.read()\n",
    "  all_topics = []\n",
    "  topics = re.findall(r'<top>(.*?)</top>', topic_content, re.DOTALL)\n",
    "  for topic in topics:\n",
    "    raw_title = re.search(r'<title>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "    title = raw_title.group(1) if raw_title else ''\n",
    "    if descriptions:\n",
    "      raw_desc = re.search(r'<desc>(.*?)\\n\\n', topic, re.DOTALL)\n",
    "      desc = raw_desc.group(1) if raw_desc else ''\n",
    "      all_topics.append({'title': title, 'description': desc})\n",
    "    else:\n",
    "      all_topics.append({'title': title})\n",
    "  return all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the topics\n",
    "topics = extract_topics('topics1-50.txt', descriptions=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_name='all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(f'sentence-transformers/{model_name}', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def search(query, model, preprocessed_documents, doc_embeddings, top_k=20):\n",
    "  query_embeddings = model.encode([query])\n",
    "  # compute distances\n",
    "  distances = scipy.spatial.distance.cdist(query_embeddings, doc_embeddings, \"cosine\")[0]\n",
    "  # get the top k results\n",
    "  results = zip(range(len(distances)), distances)\n",
    "  results = sorted(results, key=lambda x: x[1])\n",
    "  # Create a list of tuples with the document number and the distance\n",
    "  results = [(preprocessed_documents[idx].doc_no, distance) for idx, distance in results[0:top_k]]\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate doc embeddings for each document in preprocessed_documents\n",
    "doc_embeddings = model.encode([doc.doc_text for doc in preprocessed_documents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "# assuming you have a list of Document objects called documents\n",
    "# and assuming you have already populated the vector attribute of each Document object\n",
    "\n",
    "# define the headers for your CSV file\n",
    "headers = ['doc_no', 'vector']\n",
    "\n",
    "# open the CSV file in 'w' mode and write the headers\n",
    "with open(f\"embedding_saves/{model_name}.csv\", mode='w', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "  writer.writerow(headers)\n",
    "\n",
    "  # loop through each Document object and write its attributes to the CSV file\n",
    "  for x, document in enumerate(preprocessed_documents):\n",
    "    writer.writerow([document.doc_no, doc_embeddings[x]])\n",
    "\n",
    "# gzip the CSV file\n",
    "with open(f\"embedding_saves/{model_name}.csv\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.csv.gz\", 'wb') as f_out:\n",
    "    f_out.writelines(f_in)\n",
    "\n",
    "os.remove(f\"embedding_saves/{model_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embedding_saves/all-MiniLM-L6-v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# gzip the CSV file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39membedding_saves/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f_in, gzip\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membedding_saves/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.csv.gz\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f_out:\n\u001b[0;32m      3\u001b[0m     f_out\u001b[39m.\u001b[39mwritelines(f_in)\n\u001b[0;32m      5\u001b[0m os\u001b[39m.\u001b[39mremove(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membedding_saves/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\School\\CSI\\CSI4107\\Assignment 2\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'embedding_saves/all-MiniLM-L6-v2.csv'"
     ]
    }
   ],
   "source": [
    "# gzip the CSV file\n",
    "with open(f\"embedding_saves/{model_name}.csv\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.csv.gz\", 'wb') as f_out:\n",
    "    f_out.writelines(f_in)\n",
    "\n",
    "os.remove(f\"embedding_saves/{model_name}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# store the embeddings in a pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'wb') as f:\n",
    "  pickle.dump(doc_embeddings, f)\n",
    "\n",
    "# gzip the pickle file\n",
    "with open(f\"embedding_saves/{model_name}.pickle\", 'rb') as f_in, gzip.open(f\"embedding_saves/{model_name}.pickle.gz\", 'wb') as f_out:\n",
    "    f_out.writelines(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# unzip the pickle file \n",
    "with gzip.open(f\"embedding_saves/{model_name}.pickle.gz\", 'rb') as f_in:\n",
    "    doc_embeddings = pickle.load(f_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' AP880414-0081 ', 0.4201424020644551), (' AP880827-0092 ', 0.4220602943873162), (' AP880704-0017 ', 0.431396290661078), (' AP880926-0180 ', 0.46072499905200504), (' AP880607-0210 ', 0.4607841600896331), (' AP881021-0218 ', 0.4746619157266405), (' AP880608-0082 ', 0.47973523180529054), (' AP881128-0234 ', 0.48361827396065116), (' AP881006-0202 ', 0.49856607139589115), (' AP881004-0180 ', 0.5088096331984095)]\n"
     ]
    }
   ],
   "source": [
    "# Go through all the documents and search for the top 1000 results\n",
    "topic = topics[0]\n",
    "print(search(topic['title'], model, preprocessed_documents, doc_embeddings, top_k=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42b34bead2ece13e82fdd57b33000433053e25f8e38f52c0f662c8d14f00a960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
